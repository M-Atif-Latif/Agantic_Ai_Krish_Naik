{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59fc92bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9e6a20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic2.0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be866258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19fcd910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1497d488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002244CD440B0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002244CD02F00> root_client=<openai.OpenAI object at 0x000002244A779940> root_async_client=<openai.AsyncOpenAI object at 0x000002244CD02180> model_name='gpt-4o-2024-08-06' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-4o-2024-08-06\")\n",
    "print(llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6169f7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\"Agentic AI\" refers to artificial intelligence systems that are designed or behave with a degree of autonomy, capable of making decisions or performing tasks without direct human intervention. These systems exhibit a form of agency, meaning they can perceive their environment, make decisions based on their perceptions, and take actions to achieve specific goals. \\n\\nAgentic AI can be contrasted with simpler, more passive AI systems that require constant human guidance or intervention to operate. The concept of agency implies that the AI has some level of independence in processing information, learning from its environment, and adapting strategies to meet its objectives.\\n\\nSuch AI systems are often used in applications where autonomous decision-making is crucial, such as self-driving cars, autonomous drones, personal assistants, and various types of robotics, among others. The development and deployment of agentic AI raise important ethical, legal, and societal considerations, particularly regarding accountability, transparency, and control over AI decisions.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 183, 'prompt_tokens': 12, 'total_tokens': 195, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CL5wNZGW3P3SKgyvnc8SDYBJnjjTp', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--0ca91f59-9c3e-42f5-bb4b-44b5126a69fb-0' usage_metadata={'input_tokens': 12, 'output_tokens': 183, 'total_tokens': 195, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is Agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f23b93e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Agentic AI\" refers to artificial intelligence systems that are designed or behave with a degree of autonomy, capable of making decisions or performing tasks without direct human intervention. These systems exhibit a form of agency, meaning they can perceive their environment, make decisions based on their perceptions, and take actions to achieve specific goals. \n",
      "\n",
      "Agentic AI can be contrasted with simpler, more passive AI systems that require constant human guidance or intervention to operate. The concept of agency implies that the AI has some level of independence in processing information, learning from its environment, and adapting strategies to meet its objectives.\n",
      "\n",
      "Such AI systems are often used in applications where autonomous decision-making is crucial, such as self-driving cars, autonomous drones, personal assistants, and various types of robotics, among others. The development and deployment of agentic AI raise important ethical, legal, and societal considerations, particularly regarding accountability, transparency, and control over AI decisions.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "022022e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bae30045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002244D933C80>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002244D879AC0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr(''))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0216e8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002244CD440B0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002244CD02F00>, root_client=<openai.OpenAI object at 0x000002244A779940>, root_async_client=<openai.AsyncOpenAI object at 0x000002244CD02180>, model_name='gpt-4o-2024-08-06', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chaning\n",
    "chain=prompt|llm\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "491e149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a tool designed to help developers test and evaluate language model applications. It offers features such as debugging, testing, and monitoring, allowing developers to assess the performance of their language models more effectively. With Langsmith, developers can conduct detailed evaluations by utilizing an extensive dataset, implementing unit tests, and producing in-depth reports, which contribute to refining and iterating on language model applications. It's particularly valuable in ensuring that models perform as expected under various scenarios and helps in identifying areas of improvement.\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a6d0421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update, \"Langsmith\" does not correspond to any widely recognized concept, technology, or company as of October 2023. It's possible that it could be a newly emerging startup, a niche technology, or even a term used in a specific context. If Langsmith has come into prominence after October 2023, I would not be aware of its particulars.\n",
      "\n",
      "If \"Langsmith\" is a term used in specific industries or regions, or if it represents a kind of software, tool, or concept that emerged after my last update, I would recommend checking the latest online resources, business news, or technology-focused publications for the most accurate and updated information.\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2837f2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee1c7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "64734e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84dea535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is a tool designed to enhance the development of applications utilizing large language models (LLMs). It offers features to observe, evaluate, and enhance the quality of these applications.', 'features': [{'name': 'Tracking and Evaluation', 'description': 'Langsmith allows developers to track and evaluate LLM outputs, comparing them across different versions and models to ensure optimal performance.'}, {'name': 'Feedback System', 'description': 'The tool integrates a feedback system to incorporate user input, facilitating continual improvement of the application based on real-world usage.'}, {'name': 'Seamless Integration', 'description': 'Langsmith easily integrates with existing LLM applications, providing developers with a suite of tools to enhance functionality without a steep learning curve.'}], 'use_cases': ['Improving response accuracy of chatbots', 'Evaluating sentiment analysis models', 'Optimizing machine translation systems'], 'website': 'https://www.langsmith.com', 'launch_year': 2023}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e828459a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Assisgnment ---Chatprompttemplate\n",
    "\n",
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c239ee89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is a framework that provides a suite of tools and services designed to enhance the development, testing, and deployment of language models. It emphasizes efficient model training, integration capabilities, and robust performance tracking to ensure that developers can swiftly and effectively create applications using natural language processing technologies.', 'features': ['Model Training Optimization', 'Integration with Various Data Sources', 'Performance Monitoring and Evaluation Tools', 'Customizable APIs for Application Development', 'Support for Different NLP Tasks and Pipelines'], 'useCases': ['Building conversational AI applications', 'Deploying language translation tools', 'Creating sentiment analysis solutions', 'Developing text summarization services'], 'releaseDate': 'The exact release date is not specified, but it is maintained and updated regularly to keep up with the latest advancements in AI technology.', 'developer': 'Langsmith is developed by a team of AI and machine learning experts aiming to streamline the language model development process.'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e84c43",
   "metadata": {},
   "source": [
    "### Assigments: https://python.langchain.com/docs/how_to/#prompt-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81984e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f1b70e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a9990ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```xml\\n<response>\\n    <description>\\n        Langsmith is a conceptual name, often associated with proficiency and craftsmanship in language and communication. It implies a person or entity that is highly skilled in the use of language, akin to a blacksmith who shapes metal with expertise.\\n    </description>\\n    <applications>\\n        <application>\\n            It could refer to services such as writing, editing, linguistic consultancy, or language instruction.\\n        </application>\\n        <application>\\n            Langsmith may also be used in brand names or trademarks for businesses that focus on language services, content creation, or educational tools.\\n        </application>\\n        <application>\\n            It can symbolize the careful crafting of language to create impactful messages or texts.\\n        </application>\\n    </applications>\\n    <examples>\\n        <example>\\n            A company named \"Langsmith\" could offer translation and localization services to ensure effective communication in multiple languages.\\n        </example>\\n        <example>\\n            Langsmith could be a platform providing writing workshops and language resources to enhance writing skills.\\n        </example>\\n    </examples>\\n</response>\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 222, 'prompt_tokens': 173, 'total_tokens': 395, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CL6NKLRQ7u5Dusez9Jk2MwDGUVvmd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--559b445a-7144-4812-9b4f-c009171a6763-0' usage_metadata={'input_tokens': 173, 'output_tokens': 222, 'total_tokens': 395, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69ec8180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is a framework that allows developers to build applications powered by language models. It provides tools to chain together different components that use language models effectively, aimed at building custom applications for various purposes such as chatbots, question-answering systems, and more complex workflows that require interaction with external data sources. LangChain is particularly useful for creating applications that require complex reasoning, memory management, and integration with other data processing tools.</answer></response>' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 40, 'total_tokens': 133, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CL6NuyoDbh61Ynnq1LI01RjQCzbrW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--7e0d909c-c595-4e2c-a1cf-0011483f968c-0' usage_metadata={'input_tokens': 40, 'output_tokens': 93, 'total_tokens': 133, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27e244",
   "metadata": {},
   "source": [
    "# üî• Why Pydantic? Understanding Structured Data Validation in LangChain\n",
    "\n",
    "## üéØ **What is Pydantic?**\n",
    "Pydantic is a powerful Python library that provides **data validation** and **type safety** using Python type hints. It's like having a strict teacher for your data - ensuring everything is exactly what you expect!\n",
    "\n",
    "## üöÄ **Key Benefits of Using Pydantic:**\n",
    "\n",
    "### 1. **Type Safety & Validation**\n",
    "- ‚úÖ Automatically validates data types\n",
    "- ‚úÖ Catches errors before they cause problems\n",
    "- ‚úÖ Ensures consistent data structure\n",
    "\n",
    "### 2. **Schema Definition**\n",
    "- üìã Define clear data models with `BaseModel`\n",
    "- üìã Use `Field()` for detailed descriptions and constraints\n",
    "- üìã Auto-generate JSON schemas\n",
    "\n",
    "### 3. **Integration with LangChain**\n",
    "- üîó LangChain's `JsonOutputParser` works seamlessly with Pydantic\n",
    "- üîó Structured LLM outputs become Python objects\n",
    "- üîó No more manual JSON parsing!\n",
    "\n",
    "## üõ†Ô∏è **What We're Using in This Notebook:**\n",
    "\n",
    "### **Core Components:**\n",
    "1. **`BaseModel`** - Base class for creating data models\n",
    "2. **`Field()`** - Add descriptions and validation rules\n",
    "3. **`JsonOutputParser`** - Parse LLM output into structured format\n",
    "4. **`PromptTemplate`** - Create dynamic prompts with format instructions\n",
    "\n",
    "### **The Magic Workflow:**\n",
    "```\n",
    "LLM Output ‚Üí JsonOutputParser ‚Üí Pydantic Model ‚Üí Validated Python Object\n",
    "```\n",
    "\n",
    "## üí° **Real-World Benefits:**\n",
    "\n",
    "| Without Pydantic | With Pydantic |\n",
    "|------------------|----------------|\n",
    "| Manual JSON parsing | Automatic validation |\n",
    "| Runtime errors | Compile-time safety |\n",
    "| Inconsistent data | Structured models |\n",
    "| No documentation | Self-documenting code |\n",
    "\n",
    "## üé™ **Example Use Cases:**\n",
    "- **Joke Generator** (as shown below) - Structured setup/punchline\n",
    "- **User Profiles** - Name, age, email validation\n",
    "- **API Responses** - Consistent data formats\n",
    "- **Configuration Files** - Type-safe settings\n",
    "\n",
    "---\n",
    "**üî• Pro Tip:** Pydantic makes your AI applications more **reliable**, **maintainable**, and **production-ready**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8c62ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3fbf87cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why don't scientists trust atoms? Because they make up everything!\"}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7152feb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why couldn't the bicycle stand up by itself?\", punchline='Because it was two tired!')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817af8cc",
   "metadata": {},
   "source": [
    "# üìö Complete LangChain Journey: Summary & Key Learnings\n",
    "\n",
    "## üéØ **What We Accomplished in This Notebook**\n",
    "\n",
    "This comprehensive notebook covers the fundamentals of **LangChain**, **Prompt Engineering**, and **Structured Output Parsing** - essential skills for building production-ready AI applications.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß **1. Environment Setup & Configuration**\n",
    "\n",
    "### **Key Components:**\n",
    "- ‚úÖ **Environment Variables** - Secure API key management using `.env` files\n",
    "- ‚úÖ **LangSmith Integration** - For tracing and monitoring AI interactions\n",
    "- ‚úÖ **Multi-Provider Setup** - OpenAI and Groq LLM configurations\n",
    "\n",
    "### **What We Learned:**\n",
    "- How to safely manage API keys and environment variables\n",
    "- Setting up LangSmith for observability and debugging\n",
    "- Configuring multiple LLM providers for flexibility\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ **2. Large Language Models (LLMs)**\n",
    "\n",
    "### **Providers Explored:**\n",
    "- **OpenAI ChatGPT** (`gpt-4o-2024-08-06`) - High-quality, production-ready\n",
    "- **Groq** (`gemma2-9b-it`) - Fast inference, cost-effective alternative\n",
    "\n",
    "### **Core Concepts:**\n",
    "```python\n",
    "# Basic LLM Usage Pattern\n",
    "llm = ChatOpenAI(model=\"gpt-4o-2024-08-06\")\n",
    "result = llm.invoke(\"Your question here\")\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üé® **3. Prompt Engineering Mastery**\n",
    "\n",
    "### **Template Types Covered:**\n",
    "\n",
    "#### **A. ChatPromptTemplate**\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert AI Engineer\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "```\n",
    "\n",
    "#### **B. PromptTemplate** \n",
    "```python\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer: {format_instructions} {query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "```\n",
    "\n",
    "### **Benefits:**\n",
    "- üéØ **Consistent messaging** across applications\n",
    "- üîÑ **Dynamic content** with variable substitution\n",
    "- üìã **Structured responses** with format instructions\n",
    "\n",
    "---\n",
    "\n",
    "## ‚õìÔ∏è **4. LangChain Expression Language (LCEL)**\n",
    "\n",
    "### **Chain Building Patterns:**\n",
    "```python\n",
    "# Basic Chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Chain with Output Parser\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Response Generation\n",
    "response = chain.invoke({\"input\": \"Your question\"})\n",
    "```\n",
    "\n",
    "### **Key Advantages:**\n",
    "- üöÄ **Composable** - Build complex workflows from simple components\n",
    "- üîÑ **Reusable** - Create modular, maintainable code\n",
    "- üìä **Observable** - Easy debugging and monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## üì§ **5. Output Parsers - Structured Data Extraction**\n",
    "\n",
    "### **Parser Types Implemented:**\n",
    "\n",
    "| Parser Type | Use Case | Example Output |\n",
    "|-------------|----------|----------------|\n",
    "| **StrOutputParser** | Simple text extraction | Raw string content |\n",
    "| **JsonOutputParser** | Structured JSON data | `{\"key\": \"value\"}` |\n",
    "| **XMLOutputParser** | XML formatted responses | `<response><answer>...</answer></response>` |\n",
    "| **YamlOutputParser** | YAML formatted data | `setup: \"question\"\\npunchline: \"answer\"` |\n",
    "\n",
    "### **Without vs With Parsers:**\n",
    "```python\n",
    "# Without Parser - Manual handling\n",
    "raw_response = llm.invoke(\"Tell me a joke\")\n",
    "# Need to manually parse response.content\n",
    "\n",
    "# With Parser - Automatic parsing\n",
    "chain = prompt | llm | JsonOutputParser()\n",
    "parsed_data = chain.invoke({\"query\": \"Tell me a joke\"})\n",
    "# Returns structured Python object\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üõ°Ô∏è **6. Pydantic Integration - Type Safety & Validation**\n",
    "\n",
    "### **Why Pydantic?**\n",
    "- ‚úÖ **Type Safety** - Prevents runtime errors\n",
    "- ‚úÖ **Data Validation** - Ensures consistent structure\n",
    "- ‚úÖ **Self-Documentation** - Clear data models\n",
    "- ‚úÖ **IDE Support** - Better autocomplete and error detection\n",
    "\n",
    "### **Implementation Pattern:**\n",
    "```python\n",
    "# 1. Define Data Model\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "# 2. Create Parser with Model\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# 3. Build Chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# 4. Get Validated Response\n",
    "joke = chain.invoke({\"query\": \"Tell me a joke\"})\n",
    "# Returns validated Joke object with .setup and .punchline attributes\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üé™ **7. Real-World Examples & Use Cases**\n",
    "\n",
    "### **Joke Generator Application:**\n",
    "- **With Pydantic** - Structured `setup` and `punchline` fields\n",
    "- **Without Pydantic** - Unstructured JSON output\n",
    "- **YAML Format** - Alternative structured output format\n",
    "\n",
    "### **Practical Applications:**\n",
    "- üè¢ **Business Analytics** - Structured data extraction from reports\n",
    "- üßë‚Äçüíº **Customer Support** - Categorized response generation\n",
    "- üìä **Content Creation** - Template-based content with validation\n",
    "- üîç **Information Extraction** - Structured data from unstructured text\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **8. Best Practices & Production Tips**\n",
    "\n",
    "### **Security:**\n",
    "- üîê Always use `.env` files for API keys\n",
    "- üö´ Never commit secrets to version control\n",
    "- ‚úÖ Validate environment variables before use\n",
    "\n",
    "### **Error Handling:**\n",
    "- üõ°Ô∏è Implement try-catch blocks for API calls\n",
    "- üìä Use LangSmith for monitoring and debugging\n",
    "- üîÑ Add retry logic for production applications\n",
    "\n",
    "### **Performance:**\n",
    "- ‚ö° Choose appropriate LLM providers based on needs\n",
    "- üìà Use structured outputs to reduce post-processing\n",
    "- üéØ Optimize prompts for specific use cases\n",
    "\n",
    "---\n",
    "\n",
    "## üéì **Key Takeaways**\n",
    "\n",
    "1. **LangChain** simplifies LLM application development with modular components\n",
    "2. **LCEL** enables powerful chain composition with the `|` operator\n",
    "3. **Output Parsers** transform unstructured LLM responses into structured data\n",
    "4. **Pydantic** adds type safety and validation to AI applications\n",
    "5. **Prompt Engineering** is crucial for consistent, high-quality responses\n",
    "\n",
    "---\n",
    "\n",
    "## üîó **Next Steps & Further Learning**\n",
    "\n",
    "- üìö Explore LangChain Agents for complex reasoning tasks\n",
    "- üîç Learn about Vector Databases and RAG (Retrieval-Augmented Generation)\n",
    "- üåê Build web applications with Streamlit or FastAPI\n",
    "- üìä Implement comprehensive logging and monitoring\n",
    "- üß™ Experiment with different LLM providers and models\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've built a solid foundation in LangChain and are ready to create sophisticated AI applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
