{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Transitioning from Old class to New Pipe Base Operator\n",
        "\n",
        "## 1. Understanding `Runnables`\n",
        "- `Runnables` are self-contained units of work.\n",
        "- Can be executed in isolation or combined for complex operations.\n",
        "- Provides flexibility in execution (sync, async, parallel).\n",
        "\n",
        "## 2. `RunnableParallel`\n",
        "- Executes tasks concurrently.\n",
        "- Useful for performance enhancement in scenarios where tasks can run independently.\n",
        "- Syntax example:\n",
        "    ```python\n",
        "    from some_module import RunnableParallel\n",
        "    ```\n",
        "\n",
        "## 3. `RunnablePassthrough`\n",
        "- A simple `Runnable` that passes inputs directly to outputs without modification.\n",
        "- Helpful for debugging or chaining in pipelines.\n",
        "- Example use case:\n",
        "    ```python\n",
        "    from some_module import RunnablePassthrough\n",
        "    passthrough = RunnablePassthrough()\n",
        "    result = passthrough.run(input_data)\n",
        "    ```\n",
        "\n",
        "## 4. `RunnableLambda`\n",
        "- Allows quick, inline definitions of small, custom functions.\n",
        "- Example:\n",
        "    ```python\n",
        "    from some_module import RunnableLambda\n",
        "    lambda_op = RunnableLambda(lambda x: x * 2)\n",
        "    result = lambda_op.run(5)  # Output: 10\n",
        "    ```\n",
        "\n",
        "## 5. Assign Functions\n",
        "- Used to assign values or parameters during execution.\n",
        "- Useful in data pipelines to update intermediate values.\n",
        "\n",
        "## 6. Performance Improvement (Inference Speed)\n",
        "- Focus on optimizing the inference speed by leveraging parallel execution.\n",
        "- Use `RunnableParallel` or batching techniques.\n",
        "- Consider optimizing data pipelines by removing unnecessary steps.\n",
        "\n",
        "## 7. Async Invoke\n",
        "- Executes operations asynchronously, improving the overall throughput of the system.\n",
        "- Syntax example:\n",
        "    ```python\n",
        "    async def async_operation():\n",
        "        result = await some_async_function()\n",
        "    ```\n",
        "\n",
        "## 8. Batch Support\n",
        "- Handles multiple inputs at once to improve performance.\n",
        "- Can be combined with `RunnableParallel` for parallel batch execution.\n",
        "\n",
        "## 9. Async Batch Execution\n",
        "- Combines asynchronous execution with batch processing for high-performance tasks.\n",
        "- Reduces overall execution time for larger datasets.\n",
        "\n",
        "## 10. Using `Itemgetter` with `LCEL`\n",
        "- `Itemgetter` is used to extract specific items from collections.\n",
        "- When combined with `LCEL` (LangChain Execution Layer), it can streamline complex operations.\n",
        "\n",
        "## 11. Bind Tools\n",
        "- `Bind` tools help to connect different steps in the pipeline.\n",
        "- Ensures smooth data flow between various `Runnable` components.\n",
        "\n",
        "## 12. Stream Support\n",
        "- Keep your pipelines more responsive by incorporating stream support for data.\n",
        "- This allows continuous data processing and near real-time outputs.\n",
        "  \n"
      ],
      "metadata": {
        "id": "QEgRM6jCdDvH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "277yyGa0ctIm",
        "outputId": "3cf71fcb-ad02-46a8-8486-5a3da2fd1b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.72)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.4.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.9-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m992.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain_google_genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain_google_genai-2.1.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "cb9f133f842d4230946ddfd0cfbe4f88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.10)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain_community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.10)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.3.72)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.34.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (1.1.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (0.4.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (2.11.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain_huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain_huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain_huggingface) (2025.8.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (1.3.1)\n",
            "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: langchain_huggingface\n",
            "Successfully installed langchain_huggingface-0.3.1\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.3.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain_groq) (0.3.72)\n",
            "Collecting groq<1,>=0.30.0 (from langchain_groq)\n",
            "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain_groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain_groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain_groq) (4.14.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain_groq) (0.4.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain_groq) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain_groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain_groq) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain_groq) (25.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.30.0->langchain_groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain_groq) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain_groq) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain_groq) (2.32.3)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain_groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain_groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain_groq) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain_groq) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain_groq) (2.5.0)\n",
            "Downloading langchain_groq-0.3.7-py3-none-any.whl (16 kB)\n",
            "Downloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m728.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain_groq\n",
            "Successfully installed groq-0.31.0 langchain_groq-0.3.7\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_google_genai\n",
        "!pip install langchain_community\n",
        "!pip install langchain\n",
        "!pip install langchain_huggingface\n",
        "!pip install langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"]=GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "Z2UAi7ETc6xp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite-001\")"
      ],
      "metadata": {
        "id": "8HFLY1Wac6vi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "from langchain_groq import ChatGroq\n",
        "import os\n",
        "llm=ChatGroq(model_name=\"Gemma2-9b-It\")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "PUhdwYdqc6sQ",
        "outputId": "16386c33-da86-447f-edc0-4e75fd2872e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from langchain_huggingface import HuggingFaceEmbeddings\\nembeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\nfrom langchain_groq import ChatGroq\\nimport os\\nllm=ChatGroq(model_name=\"Gemma2-9b-It\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is my simple chain (old chaining concept)"
      ],
      "metadata": {
        "id": "3BZUYXLZf--F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template= 'Hi! I am learning {skill}. Can you suggest me top 5 things to learn?\\n'"
      ],
      "metadata": {
        "id": "dLSBP719c6op"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "kC5AOFIAc6ks"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(template=template,input_variables=[\"skill\"])"
      ],
      "metadata": {
        "id": "sMwbaFDmc6hB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn0dSKJVc6dR",
        "outputId": "0664078f-ba9e-415e-cf72-34401388617d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['skill'] input_types={} partial_variables={} template='Hi! I am learning {skill}. Can you suggest me top 5 things to learn?\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "llm_chain = LLMChain(prompt=prompt,llm=llm)\n",
        "print(llm_chain.run('Data Science'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQcFyFRWc6Zk",
        "outputId": "8375a31c-7b63-4102-cf9b-fd6d86d24d62"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2026417700.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(prompt=prompt,llm=llm)\n",
            "/tmp/ipython-input-2026417700.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  print(llm_chain.run('Data Science'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That's great! Data Science is a fascinating and rapidly growing field. Here are the top 5 things I recommend you learn, along with some context and resources:\n",
            "\n",
            "1.  **Programming Fundamentals (Python or R):** This is the absolute bedrock. You need a programming language to manipulate data, build models, and automate tasks.\n",
            "\n",
            "    *   **Why:** Data science heavily relies on code. Python and R are the dominant languages.\n",
            "    *   **What to Learn:**\n",
            "        *   **Variables and Data Types:** Integers, floats, strings, booleans.\n",
            "        *   **Data Structures:** Lists, dictionaries (Python) or lists, data frames (R).\n",
            "        *   **Control Flow:** `if/else` statements, `for` and `while` loops.\n",
            "        *   **Functions:** Defining and using functions to modularize your code.\n",
            "        *   **Libraries:**  You'll need to learn specific libraries, but start with the basics.\n",
            "    *   **Resources:**\n",
            "        *   **Python:**\n",
            "            *   **Codecademy:**  [https://www.codecademy.com/learn/learn-python-3](https://www.codecademy.com/learn/learn-python-3) (Interactive, good for beginners)\n",
            "            *   **Google's Python Class:** [https://developers.google.com/edu/python](https://developers.google.com/edu/python) (More in-depth, free)\n",
            "            *   **DataCamp:** [https://www.datacamp.com/](https://www.datacamp.com/) (Focuses on data science with Python)\n",
            "        *   **R:**\n",
            "            *   **Codecademy:** [https://www.codecademy.com/learn/learn-r](https://www.codecademy.com/learn/learn-r) (Interactive)\n",
            "            *   **DataCamp:** [https://www.datacamp.com/](https://www.datacamp.com/)\n",
            "            *   **R for Data Science:** [https://r4ds.hadley.nz/](https://r4ds.hadley.nz/) (Free online book, excellent for data analysis)\n",
            "\n",
            "2.  **Data Manipulation and Analysis (Pandas/dplyr):**  Once you have the programming basics, you need to learn how to clean, transform, and analyze data. This is where libraries like Pandas (Python) and `dplyr` (R) come in.\n",
            "\n",
            "    *   **Why:**  Data is rarely \"clean\" when you get it. You'll spend a significant amount of time wrangling data.\n",
            "    *   **What to Learn:**\n",
            "        *   **DataFrames:**  Understanding how data is structured in tables (rows and columns).\n",
            "        *   **Data Cleaning:** Handling missing values, removing duplicates, correcting errors.\n",
            "        *   **Data Transformation:** Filtering, sorting, grouping, creating new columns.\n",
            "        *   **Data Aggregation:** Calculating summary statistics (mean, median, standard deviation, etc.).\n",
            "        *   **Merging and Joining Data:** Combining data from multiple sources.\n",
            "    *   **Resources:**\n",
            "        *   **Pandas (Python):**\n",
            "            *   **Pandas Documentation:** [https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/) (Comprehensive, but can be overwhelming initially)\n",
            "            *   **\"Python for Data Analysis\" by Wes McKinney (Creator of Pandas):**  A good book.\n",
            "            *   **Kaggle Tutorials:** [https://www.kaggle.com/learn/pandas](https://www.kaggle.com/learn/pandas) (Interactive, practical)\n",
            "        *   **dplyr (R):**\n",
            "            *   **dplyr Documentation:** [https://dplyr.tidyverse.org/](https://dplyr.tidyverse.org/)\n",
            "            *   **R for Data Science:** [https://r4ds.hadley.nz/](https://r4ds.hadley.nz/) (Excellent coverage of `dplyr`)\n",
            "\n",
            "3.  **Data Visualization:**  The ability to communicate your findings effectively is crucial.  Data visualization helps you explore data and present insights to others.\n",
            "\n",
            "    *   **Why:**  Visuals make complex data easier to understand and can reveal patterns that are not obvious in tables.\n",
            "    *   **What to Learn:**\n",
            "        *   **Basic Chart Types:** Histograms, scatter plots, bar charts, box plots, line graphs.\n",
            "        *   **Advanced Chart Types:** Heatmaps, density plots, etc. (As you progress)\n",
            "        *   **Libraries:**\n",
            "            *   **Python:** `matplotlib`, `seaborn`, `plotly`\n",
            "            *   **R:** `ggplot2` (the most popular), `plotly`\n",
            "        *   **Principles of Good Visualization:**  Choosing the right chart for the data, avoiding misleading visuals, clear labeling.\n",
            "    *   **Resources:**\n",
            "        *   **Python:**\n",
            "            *   **Matplotlib Documentation:** [https://matplotlib.org/stable/](https://matplotlib.org/stable/)\n",
            "            *   **Seaborn Documentation:** [https://seaborn.pydata.org/](https://seaborn.pydata.org/)\n",
            "            *   **Plotly Documentation:** [https://plotly.com/python/](https://plotly.com/python/)\n",
            "            *   **DataCamp:** [https://www.datacamp.com/](https://www.datacamp.com/) (Courses for all libraries)\n",
            "        *   **R:**\n",
            "            *   **ggplot2 Documentation:** [https://ggplot2.tidyverse.org/](https://ggplot2.tidyverse.org/)\n",
            "            *   **R for Data Science:** [https://r4ds.hadley.nz/](https://r4ds.hadley.nz/) (Excellent coverage of `ggplot2`)\n",
            "            *   **DataCamp:** [https://www.datacamp.com/](https://www.datacamp.com/)\n",
            "\n",
            "4.  **Statistics and Probability:**  A solid understanding of statistical concepts is essential for interpreting data and building reliable models.\n",
            "\n",
            "    *   **Why:**  Data science is deeply rooted in statistics.  You'll need to understand concepts like distributions, hypothesis testing, and regression.\n",
            "    *   **What to Learn:**\n",
            "        *   **Descriptive Statistics:** Mean, median, mode, standard deviation, variance.\n",
            "        *   **Probability Distributions:** Normal distribution, binomial distribution, Poisson distribution.\n",
            "        *   **Inferential Statistics:** Hypothesis testing (t-tests, chi-squared tests), confidence intervals.\n",
            "        *   **Regression:** Linear regression, understanding model coefficients, R-squared.\n",
            "        *   **Correlation and Causation:**  Understanding the difference between correlation and causation.\n",
            "    *   **Resources:**\n",
            "        *   **Khan Academy:** [https://www.khanacademy.org/math/statistics-probability](https://www.khanacademy.org/math/statistics-probability) (Excellent free resource)\n",
            "        *   **StatQuest with Josh Starmer (YouTube):**  [https://statquest.org/](https://statquest.org/) (Clear and intuitive explanations of statistical concepts)\n",
            "        *   **\"Statistics for Dummies\"** (or similar beginner-friendly books)\n",
            "        *   **MIT OpenCourseware:** Search for introductory statistics courses.\n",
            "\n",
            "5.  **Machine Learning Fundamentals:**  Machine learning is a core component of data science.\n",
            "\n",
            "    *   **Why:**  Machine learning algorithms can automate tasks, make predictions, and discover patterns in data.\n",
            "    *   **What to Learn:**\n",
            "        *   **Supervised Learning:**\n",
            "            *   **Regression:** Linear regression, logistic regression.\n",
            "            *   **Classification:** K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Trees, Random Forests.\n",
            "        *   **Unsupervised Learning:**\n",
            "            *   **Clustering:** K-Means, Hierarchical Clustering.\n",
            "            *   **Dimensionality Reduction:** Principal Component Analysis (PCA).\n",
            "        *   **Model Evaluation:** Accuracy, precision, recall, F1-score, AUC-ROC (for classification);  MAE, MSE, RMSE (for regression).\n",
            "    *   **Resources:**\n",
            "        *   **Scikit-learn Documentation (Python):** [https://scikit-learn.org/stable/](https://scikit-learn.org/stable/) (The go-to library for machine learning in Python)\n",
            "        *   **\"Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow\" by Aurélien Géron:**  A popular and practical book.\n",
            "        *   **Machine Learning by Andrew Ng (Coursera):** [https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning) (A classic, highly recommended)\n",
            "        *   **R Packages:** `caret`, `mlr` (For machine learning in R)\n",
            "        *   **Kaggle:** Participate in Kaggle competitions to practice machine learning.\n",
            "\n",
            "**Important Tips for Success:**\n",
            "\n",
            "*   **Start Small and Build Gradually:** Don't try to learn everything at once. Focus on the fundamentals first.\n",
            "*   **Practice, Practice, Practice:** The best way to learn is by doing. Work on projects, even small ones.\n",
            "*   **Use Real-World Datasets:** Find datasets on Kaggle, UCI Machine Learning Repository, or other sources.\n",
            "*   **Join a Community:** Connect with other learners online or in person. Ask questions and share your progress.\n",
            "*   **Don't Be Afraid to Make Mistakes:** Everyone makes mistakes. Learn from them and keep going.\n",
            "*   **Stay Curious:** Data science is constantly evolving. Keep learning and exploring new tools and techniques.\n",
            "*   **Focus on Projects:**  Build projects to demonstrate your skills.  This is more valuable than just taking courses.\n",
            "\n",
            "Good luck on your data science journey!  Let me know if you have any other questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain.run({'skill':'Data Science'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSRxFChEc6Vs",
        "outputId": "87155b87-f752-4119-cd46-d408878ee7ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That's fantastic! Data Science is a rewarding field. Here are the top 5 things I recommend you focus on when starting out, along with explanations and how to approach learning them:\n",
            "\n",
            "1.  **Python Programming:**\n",
            "\n",
            "    *   **Why it's crucial:** Python is the dominant language in data science. It has a vast ecosystem of libraries specifically designed for data manipulation, analysis, and machine learning.  It's relatively easy to learn and has excellent community support.\n",
            "    *   **What to learn:**\n",
            "        *   **Fundamentals:** Variables, data types (integers, floats, strings, booleans, lists, dictionaries, tuples), operators, control flow (if/else statements, loops), functions, and object-oriented programming (classes and objects - start with the basics).\n",
            "        *   **Key libraries:**\n",
            "            *   **NumPy:** For numerical computing, array operations (essential for handling large datasets).\n",
            "            *   **Pandas:** For data manipulation and analysis (data cleaning, transformation, and exploration using DataFrames).\n",
            "            *   **Matplotlib/Seaborn:** For data visualization (creating charts and graphs to understand your data).\n",
            "        *   **How to learn:**\n",
            "            *   **Online courses:**  Platforms like Codecademy, DataCamp, Coursera (e.g., Google's Data Analytics Professional Certificate), and edX offer excellent Python for data science courses.\n",
            "            *   **Interactive tutorials:** Websites like Kaggle provide interactive Python tutorials and practice exercises, often within a data science context.\n",
            "            *   **Practice:** The best way to learn is by doing.  Work through exercises, solve coding challenges (e.g., on HackerRank or LeetCode - start with easy problems), and try simple projects.\n",
            "\n",
            "2.  **Data Manipulation and Cleaning (Using Pandas):**\n",
            "\n",
            "    *   **Why it's crucial:**  Real-world data is almost always messy.  Data cleaning is a critical step in the data science pipeline.  You'll spend a significant amount of time on this.\n",
            "    *   **What to learn:**\n",
            "        *   **Data loading:**  Reading data from various sources (CSV, Excel, databases, web APIs).\n",
            "        *   **Data exploration:** Understanding the structure of your data (columns, data types, missing values, summary statistics).\n",
            "        *   **Data cleaning:** Handling missing values (imputation), removing duplicates, dealing with outliers, correcting errors.\n",
            "        *   **Data transformation:**  Filtering, sorting, grouping, aggregation, creating new columns, merging/joining datasets.\n",
            "    *   **How to learn:**\n",
            "        *   **Pandas documentation:**  The official Pandas documentation is a great resource.\n",
            "        *   **Online courses:**  Focus on Pandas-specific courses within the Python courses mentioned above.\n",
            "        *   **Kaggle datasets:**  Practice on real-world datasets from Kaggle.  Look for \"Titanic\" or \"House Prices\" datasets to start.  Work through the provided notebooks to see how others clean and analyze the data.\n",
            "\n",
            "3.  **Data Visualization:**\n",
            "\n",
            "    *   **Why it's crucial:**  Visualization helps you understand your data, identify patterns, communicate your findings effectively, and debug your models.\n",
            "    *   **What to learn:**\n",
            "        *   **Basic chart types:** Histograms, scatter plots, bar charts, line plots, box plots, heatmaps.\n",
            "        *   **Choosing the right chart:** Understanding which chart type is appropriate for different types of data and insights.\n",
            "        *   **Customization:**  Labels, titles, legends, colors, and other elements to make your visualizations clear and informative.\n",
            "        *   **Tools:** Matplotlib, Seaborn (built on Matplotlib and provides higher-level visualizations and a more aesthetically pleasing look), and potentially other tools like Plotly or Bokeh (for interactive visualizations).\n",
            "    *   **How to learn:**\n",
            "        *   **Online courses:**  Look for visualization-specific modules within your Python courses.\n",
            "        *   **Seaborn documentation:**  Seaborn's documentation has excellent examples.\n",
            "        *   **Practice:**  Create visualizations for your projects and datasets.  Experiment with different chart types and customizations.\n",
            "        *   **Learn from others:**  Look at visualizations created by data scientists on platforms like Kaggle and blogs.\n",
            "\n",
            "4.  **Statistical Concepts:**\n",
            "\n",
            "    *   **Why it's crucial:** Understanding statistical concepts is fundamental to interpreting your data, building effective models, and drawing valid conclusions.\n",
            "    *   **What to learn:**\n",
            "        *   **Descriptive Statistics:** Mean, median, mode, standard deviation, variance, percentiles, quartiles.\n",
            "        *   **Probability:**  Basic probability, probability distributions (normal, binomial, etc.).\n",
            "        *   **Inferential Statistics:** Hypothesis testing (t-tests, chi-squared tests), confidence intervals, p-values.\n",
            "        *   **Correlation and Regression:**  Understanding relationships between variables.  Linear regression.\n",
            "        *   **Exploratory Data Analysis (EDA):**  Techniques for summarizing and visualizing your data to gain initial insights.\n",
            "    *   **How to learn:**\n",
            "        *   **Online courses:**  Look for statistics courses on platforms like Khan Academy, Coursera (e.g.,  Statistics with R Specialization from Duke University), edX, or Udacity.\n",
            "        *   **Textbooks:**  \"Think Stats\" by Allen B. Downey is a good, practical introduction.\n",
            "        *   **Practice:**  Apply statistical concepts to your data analysis projects.  Calculate descriptive statistics, perform hypothesis tests, and interpret the results.\n",
            "\n",
            "5.  **Machine Learning Fundamentals:**\n",
            "\n",
            "    *   **Why it's crucial:** Machine learning is a core component of many data science applications.  Understanding the basics will enable you to build predictive models and solve real-world problems.\n",
            "    *   **What to learn:**\n",
            "        *   **Supervised Learning:**  Classification (e.g., predicting customer churn, classifying emails as spam), Regression (e.g., predicting house prices, forecasting sales).  Learn about different algorithms like:\n",
            "            *   Linear Regression\n",
            "            *   Logistic Regression\n",
            "            *   Decision Trees\n",
            "            *   Random Forests\n",
            "            *   Support Vector Machines (SVMs)\n",
            "        *   **Unsupervised Learning:** Clustering (e.g., customer segmentation), Dimensionality Reduction (e.g., reducing the number of features). Learn about different algorithms like:\n",
            "            *   K-Means Clustering\n",
            "            *   Principal Component Analysis (PCA)\n",
            "        *   **Model Evaluation:** Accuracy, precision, recall, F1-score, ROC curves (for classification); Mean Squared Error (MSE), R-squared (for regression).\n",
            "        *   **Model Selection:** Choosing the right model for your problem.\n",
            "        *   **Scikit-learn:**  This is the main Python library for machine learning. Learn how to use it.\n",
            "    *   **How to learn:**\n",
            "        *   **Online courses:**  Coursera (e.g., Machine Learning by Andrew Ng), edX, Udacity, and DataCamp offer excellent courses.\n",
            "        *   **Scikit-learn documentation:**  The Scikit-learn documentation provides tutorials and examples.\n",
            "        *   **Kaggle competitions:**  Participate in Kaggle competitions (even if you don't win) to practice building and evaluating models.\n",
            "        *   **Practice:**  Apply machine learning algorithms to real-world datasets.\n",
            "\n",
            "**Tips for Success:**\n",
            "\n",
            "*   **Start with the basics:** Don't try to learn everything at once. Build a solid foundation in Python and data manipulation before diving into more advanced topics.\n",
            "*   **Practice, practice, practice:** The more you practice, the better you'll become. Work through exercises, complete projects, and participate in coding challenges.\n",
            "*   **Build projects:**  Create your own data science projects to apply what you've learned. This is the best way to solidify your understanding and build your portfolio.\n",
            "*   **Join a community:**  Connect with other data science learners and professionals.  Join online forums, attend meetups, and ask questions.  The data science community is very supportive.\n",
            "*   **Don't be afraid to ask for help:**  When you get stuck, don't hesitate to ask for help from online communities, forums, or instructors.\n",
            "*   **Stay curious and keep learning:** Data science is a rapidly evolving field.  Stay up-to-date on the latest technologies and trends.\n",
            "\n",
            "Good luck on your data science journey!  It's a challenging but incredibly rewarding field.  Let me know if you have any more questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is a implementation  using LCEL"
      ],
      "metadata": {
        "id": "UCdmC32vhBJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtHhtmCgc6SJ",
        "outputId": "2459501a-5a79-42de-e627-a3b7cd476798"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='models/gemini-2.0-flash-lite-001', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7a503f96b2d0>, default_metadata=(), model_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBICKLhfc5x_",
        "outputId": "9400dd43-b203-40d6-dd46-df637fbc0c24"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['skill'], input_types={}, partial_variables={}, template='Hi! I am learning {skill}. Can you suggest me top 5 things to learn?\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "XK8YOGlmc5tc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({'skill':'Big Data'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYqz00LOc5r3",
        "outputId": "65a73efa-45a8-4bbb-b40d-16d7bc5686e8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Okay, diving into the world of Big Data can feel overwhelming, but here\\'s a recommended top 5 list of things to learn, focusing on practical skills and a solid foundation:\\n\\n1.  **Data Storage and Processing Fundamentals (Hadoop Ecosystem):**\\n\\n    *   **Why it\\'s crucial:**  You need to understand how to store and process massive datasets. The Hadoop ecosystem (or its modern alternatives) is the cornerstone of this.\\n    *   **What to learn:**\\n        *   **Hadoop Distributed File System (HDFS):** Learn how data is stored across multiple machines (nodes) for redundancy and scalability. Understand concepts like block sizes, replication, and data locality.\\n        *   **MapReduce (or its modern replacement):**  Grasp the basic idea of breaking down large computations into parallel tasks. While MapReduce itself is often considered \"legacy,\" understanding the concept is vital to understanding how distributed processing works.  The newer frameworks like Spark are built on similar principles.\\n        *   **YARN (Yet Another Resource Negotiator):**  Learn how resources (CPU, memory) are managed and allocated within a Hadoop cluster.\\n        *   **Relevant Modern Technologies:**\\n            *   **Apache Spark:**  This is a *must-learn* today. It\\'s a fast, in-memory processing engine that\\'s widely used. Learn its core concepts (RDDs, DataFrames, Spark SQL), and how to write Spark applications in Python or Scala.\\n            *   **Apache Hive:**  A data warehouse system built on Hadoop. Learn how to use HiveQL (SQL-like language) to query and analyze data stored in HDFS.\\n            *   **Apache Kafka:** A distributed streaming platform for real-time data ingestion and processing.\\n\\n2.  **Programming Languages for Big Data:**\\n\\n    *   **Why it\\'s crucial:** You\\'ll need a language to interact with your data, perform transformations, and build data pipelines.\\n    *   **What to learn:**\\n        *   **Python:**  This is *the* dominant language in the Big Data world, alongside Java and Scala. It\\'s easy to learn, has a vast ecosystem of libraries (like Pandas, Scikit-learn, Spark), and is used for data analysis, machine learning, and more.\\n        *   **SQL:**  Regardless of the specific data technologies you use, SQL is essential for querying and manipulating data.  Learn SQL syntax, joins, aggregations, and window functions.  Many Big Data tools (like Hive, Spark SQL, and Presto) support SQL-like query languages.\\n        *   **Scala (Optional but valuable):** Scala is the primary language used for developing Spark applications.  It\\'s a more complex language than Python, but it offers performance advantages and a strong functional programming paradigm. If you want to be a Spark developer, Scala is a must.\\n\\n3.  **Data Warehousing and Data Lakes:**\\n\\n    *   **Why it\\'s crucial:** Big Data often involves storing and analyzing data from various sources. Data warehouses and data lakes provide the infrastructure for this.\\n    *   **What to learn:**\\n        *   **Data Warehousing Concepts:** Understand the principles of data warehousing, including dimensional modeling (star schemas, snowflake schemas), ETL (Extract, Transform, Load) processes, and the benefits of structured data.\\n        *   **Data Lakes:** Learn about the concepts of data lakes, which store raw and unstructured data. Understand the advantages and disadvantages of data lakes compared to data warehouses.\\n        *   **Cloud Data Warehouses:** Learn about popular cloud-based data warehouses like Amazon Redshift, Google BigQuery, and Snowflake. These offer scalability and ease of use.\\n        *   **Data Lake Technologies:** Learn about technologies for building and managing data lakes, such as Apache Parquet (columnar storage format), Apache Iceberg, and Delta Lake.\\n\\n4.  **Data Pipelines and Workflow Management:**\\n\\n    *   **Why it\\'s crucial:**  You need to automate the process of collecting, processing, and moving data from one place to another.\\n    *   **What to learn:**\\n        *   **ETL/ELT Processes:** Understand how to extract data from various sources, transform it (clean, format, aggregate), and load it into a data warehouse or data lake.\\n        *   **Workflow Management Tools:**  Learn about tools that orchestrate data pipelines and manage dependencies. Popular options include:\\n            *   **Apache Airflow:** A widely used, open-source platform for creating, scheduling, and monitoring data pipelines.\\n            *   **Apache NiFi:** A data flow system for automating the movement and transformation of data.\\n            *   **Cloud-Based Workflow Services:**  Learn about the workflow services offered by cloud providers like AWS (e.g., AWS Glue, Step Functions), Google Cloud (e.g., Cloud Composer), and Azure (e.g., Azure Data Factory).\\n\\n5.  **Cloud Computing Fundamentals:**\\n\\n    *   **Why it\\'s crucial:**  Cloud platforms (AWS, Google Cloud, Azure) provide the infrastructure (storage, compute, services) that\\'s increasingly used for Big Data.\\n    *   **What to learn:**\\n        *   **Cloud Provider Basics:**  Familiarize yourself with the basic services offered by a major cloud provider (e.g., AWS, Google Cloud, Azure).  This includes:\\n            *   **Compute:**  Virtual machines (EC2 on AWS, Compute Engine on GCP, Virtual Machines on Azure), serverless computing (Lambda on AWS, Cloud Functions on GCP, Azure Functions).\\n            *   **Storage:**  Object storage (S3 on AWS, Cloud Storage on GCP, Azure Blob Storage), databases (RDS on AWS, Cloud SQL on GCP, Azure SQL Database).\\n            *   **Data Warehousing/Analytics Services:** Services like Amazon Redshift, Google BigQuery, and Azure Synapse Analytics.\\n        *   **Cloud Data Services:** Learn how to use cloud-specific services for data storage, processing, and analysis.  Examples:\\n            *   **AWS:**  S3, EMR (Elastic MapReduce - Hadoop/Spark), Glue, Athena.\\n            *   **Google Cloud:**  Cloud Storage, Dataproc (Hadoop/Spark), BigQuery, Dataflow, Cloud Composer.\\n            *   **Azure:**  Azure Data Lake Storage, Azure HDInsight (Hadoop/Spark), Azure Synapse Analytics, Azure Data Factory.\\n        *   **Cost Optimization:**  Understand how to manage costs in the cloud.\\n\\n**Important Considerations and Tips:**\\n\\n*   **Start Small and Iterate:** Don\\'t try to learn everything at once. Focus on the core concepts and build up your knowledge gradually.\\n*   **Hands-on Practice:** The most important thing is to *practice*. Work through tutorials, build small projects, and experiment with different technologies.\\n*   **Choose a Project:** Find a real-world problem or dataset that interests you.  This will motivate you to learn and give you a practical application for your skills.\\n*   **Online Resources:**  Utilize online resources like:\\n    *   **Online Courses:** Coursera, edX, Udacity, DataCamp, Udemy offer excellent Big Data courses.\\n    *   **Documentation:**  Read the official documentation for the technologies you\\'re learning.\\n    *   **Blogs and Communities:**  Follow blogs and participate in online communities (Stack Overflow, Reddit) to learn from others and get help.\\n*   **Stay Updated:**  The Big Data landscape is constantly evolving.  Be prepared to learn new technologies and adapt to changes.\\n\\nGood luck!  This is a challenging but rewarding field. Let me know if you have any more questions!' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite-001', 'safety_ratings': []} id='run--ad12369f-9345-4c56-8d8c-890f12a306f0-0' usage_metadata={'input_tokens': 20, 'output_tokens': 1632, 'total_tokens': 1652, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "3pIWy2zac5oY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "KkdsOPU5c5kp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | parser"
      ],
      "metadata": {
        "id": "nY-yOdtJc5hB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({'skill':'Machine Learning'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLAf8TKXc5dQ",
        "outputId": "80cd678e-13d9-4814-fde3-e95239d03dcb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That's fantastic! Machine learning is a fascinating field. Here are the top 5 things I recommend you focus on when starting out, in a suggested order of learning:\n",
            "\n",
            "1.  **Python Programming Fundamentals:**\n",
            "    *   **Why it's essential:** Python is the dominant language in machine learning.  You'll need it to write your code, manipulate data, and build models.\n",
            "    *   **What to learn:**\n",
            "        *   **Basic syntax:** Variables, data types (integers, floats, strings, booleans, lists, dictionaries), operators.\n",
            "        *   **Control flow:** `if/else` statements, `for` and `while` loops.\n",
            "        *   **Functions:**  Defining and calling functions, understanding arguments and return values.\n",
            "        *   **Data structures:** Lists, dictionaries, tuples, sets (how to use, access elements, and manipulate them).\n",
            "        *   **Object-Oriented Programming (OOP) Basics (optional, but helpful):** Classes, objects, methods, inheritance.\n",
            "        *   **Libraries:**  Get familiar with how to install and import libraries using `pip`.\n",
            "\n",
            "    *   **Resources:**\n",
            "        *   **Codecademy:** (Python 3 course) - excellent interactive learning.\n",
            "        *   **FreeCodeCamp:** (Python for Beginners course) - great video tutorials and projects.\n",
            "        *   **Official Python Tutorial:** - the most authoritative and in-depth.\n",
            "        *   **Google's Python Class:** - a more structured, classroom-style resource.\n",
            "        *   **Kaggle's Python Course:** - specifically tailored for data science.\n",
            "\n",
            "2.  **Core Machine Learning Concepts:**\n",
            "    *   **Why it's essential:**  This gives you the big picture. You need to understand the principles before you start coding.\n",
            "    *   **What to learn:**\n",
            "        *   **Types of Machine Learning:**  Supervised learning (regression, classification), unsupervised learning (clustering, dimensionality reduction), reinforcement learning (this can come later).\n",
            "        *   **Key terms:**  Training data, test data, features, labels, model, prediction, overfitting, underfitting, bias-variance tradeoff.\n",
            "        *   **Algorithms:**  A basic understanding of how they work (don't need to know the math immediately, but the idea).  Start with these:\n",
            "            *   **Linear Regression:**  Predicting a continuous value.\n",
            "            *   **Logistic Regression:**  Classifying data into categories.\n",
            "            *   **K-Nearest Neighbors (KNN):**  Classification and regression based on proximity.\n",
            "            *   **Decision Trees:**  Creating a tree-like structure for decisions.\n",
            "            *   **Clustering (K-Means):** Grouping data points.\n",
            "        *   **Evaluation Metrics:** How to measure how well your models are performing (accuracy, precision, recall, F1-score, mean squared error, etc.).\n",
            "        *   **Data Preprocessing:**  Cleaning data, handling missing values, feature scaling.\n",
            "\n",
            "    *   **Resources:**\n",
            "        *   **Machine Learning by Andrew Ng (Coursera):** The gold standard for a comprehensive introduction.  (Free audit option available.)\n",
            "        *   **Machine Learning Mastery:**  (Blog and resources) - Excellent explanations and code examples.\n",
            "        *   **Scikit-learn documentation:** (Python library) - a fantastic reference for algorithms and techniques.\n",
            "        *   **StatQuest with Josh Starmer:**  (YouTube channel) - Clear and engaging explanations of ML concepts (with good visuals).\n",
            "\n",
            "3.  **NumPy and Pandas:**\n",
            "    *   **Why it's essential:** These are the workhorses for data manipulation and numerical computation in Python.\n",
            "    *   **What to learn:**\n",
            "        *   **NumPy:**\n",
            "            *   Creating and manipulating NumPy arrays (the foundation for numerical operations).\n",
            "            *   Array indexing, slicing, and reshaping.\n",
            "            *   Mathematical operations on arrays (addition, subtraction, dot product, etc.).\n",
            "        *   **Pandas:**\n",
            "            *   Creating and working with DataFrames (tables).\n",
            "            *   Data loading and saving (from CSV, Excel, etc.).\n",
            "            *   Data cleaning (handling missing values, filtering).\n",
            "            *   Data manipulation (adding/removing columns, sorting, grouping, merging).\n",
            "            *   Data analysis (descriptive statistics, basic plotting).\n",
            "\n",
            "    *   **Resources:**\n",
            "        *   **NumPy documentation:** The official documentation is comprehensive.\n",
            "        *   **Pandas documentation:** Same as above, but for Pandas.\n",
            "        *   **Kaggle's Pandas Course:**  Interactive and practical.\n",
            "        *   **\"Python for Data Analysis\" by Wes McKinney (O'Reilly):** The definitive book on Pandas (the creator of Pandas).\n",
            "\n",
            "4.  **Scikit-learn:**\n",
            "    *   **Why it's essential:** This is *the* go-to Python library for machine learning. It provides a vast collection of pre-built algorithms, tools for model selection, and evaluation metrics.\n",
            "    *   **What to learn:**\n",
            "        *   **How to use Scikit-learn:**  The general workflow:\n",
            "            *   Importing the necessary modules (e.g., `from sklearn.linear_model import LinearRegression`).\n",
            "            *   Creating a model instance (e.g., `model = LinearRegression()`).\n",
            "            *   Fitting the model to your data (`model.fit(X_train, y_train)`).\n",
            "            *   Making predictions (`y_pred = model.predict(X_test)`).\n",
            "            *   Evaluating the model (`accuracy_score`, `mean_squared_error`, etc.).\n",
            "        *   **Key modules:**\n",
            "            *   `sklearn.model_selection`:  Splitting data, cross-validation, hyperparameter tuning.\n",
            "            *   `sklearn.preprocessing`: Feature scaling, encoding categorical variables.\n",
            "            *   `sklearn.linear_model`:  Linear models (linear regression, logistic regression, etc.).\n",
            "            *   `sklearn.neighbors`:  K-Nearest Neighbors.\n",
            "            *   `sklearn.tree`: Decision trees.\n",
            "            *   `sklearn.cluster`:  Clustering algorithms (K-Means, etc.).\n",
            "        *   **Hyperparameter Tuning:**  Understanding how to optimize the settings of your models (e.g., the number of neighbors in KNN, the regularization strength in linear regression).\n",
            "\n",
            "    *   **Resources:**\n",
            "        *   **Scikit-learn documentation:** Extremely well-written, with examples.\n",
            "        *   **Scikit-learn tutorials:**  The documentation provides many tutorials, which are excellent.\n",
            "        *   **Kaggle kernels:**  Browse and learn from other people's code.\n",
            "\n",
            "5.  **Data Visualization:**\n",
            "    *   **Why it's essential:**  Visualizing your data and model results is crucial for understanding your data, identifying patterns, and communicating your findings.\n",
            "    *   **What to learn:**\n",
            "        *   **Matplotlib:**  The foundational plotting library in Python.\n",
            "        *   **Seaborn:**  Built on Matplotlib, provides a higher-level interface for creating more attractive and informative statistical plots.\n",
            "        *   **Basic plot types:**  Histograms, scatter plots, line plots, bar charts, box plots.\n",
            "        *   **Customization:**  Adding labels, titles, legends, and controlling the appearance of your plots.\n",
            "\n",
            "    *   **Resources:**\n",
            "        *   **Matplotlib documentation:**  The official documentation.\n",
            "        *   **Seaborn documentation:**  The official documentation.\n",
            "        *   **DataCamp:** (Courses on data visualization with Python) - good interactive learning.\n",
            "        *   **Kaggle kernels:**  See how others create visualizations.\n",
            "\n",
            "**Important Considerations:**\n",
            "\n",
            "*   **Practice, Practice, Practice:** The best way to learn is by doing.  Work through tutorials, complete exercises, and build your own projects.\n",
            "*   **Start with Small Projects:** Don't try to solve world hunger right away. Begin with simple datasets and problems.\n",
            "*   **Don't Be Afraid to Ask for Help:** Use online forums (Stack Overflow, Reddit's r/learnmachinelearning, etc.) to ask questions.\n",
            "*   **Stay Curious:** Machine learning is constantly evolving. Keep learning and exploring new techniques.\n",
            "*   **Consider a Project:** After working your way through the above, pick a simple project, such as classifying Iris flowers or predicting house prices.\n",
            "\n",
            "Good luck on your machine learning journey!  It's a challenging but incredibly rewarding field. Let me know if you have any other questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets discuss about the runnables"
      ],
      "metadata": {
        "id": "ciOsYr0qhr_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough , RunnableLambda"
      ],
      "metadata": {
        "id": "XGo6_AIoc5Zo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnablePassthrough()"
      ],
      "metadata": {
        "id": "mFHVq8IphoG5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke('Welcome to this youtube channel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VoEqNMkUhoAS",
        "outputId": "d67da2d4-e625-4768-d989-19b809589c9f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Welcome to this youtube channel'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnablePassthrough() | RunnablePassthrough() | RunnablePassthrough()"
      ],
      "metadata": {
        "id": "KArTsaMshn8A"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke('Welcome to my Atif\"s youtube channel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u69CUyJBhn3z",
        "outputId": "1bc4748e-2337-4827-f97b-7df6cbf21cf5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Welcome to my Atif\"s youtube channel'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def string_upper(input):\n",
        "  return input.upper()"
      ],
      "metadata": {
        "id": "OA0qMugshn0J"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RunnablePassthrough() | RunnableLambda(string_upper)"
      ],
      "metadata": {
        "id": "7TbZRS2Zhnwo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke('Welcome to my Atif\"s youtube channel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VTMDBYtghntr",
        "outputId": "ef05437e-6034-477b-f09d-74e9e16794e8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'WELCOME TO MY ATIF\"S YOUTUBE CHANNEL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdqbsQgYih51",
        "outputId": "9d3f3bbb-6f83-46c0-fef8-62f90035ccd4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.16-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.1)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.4)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m477.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.11.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.25.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.16-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=5e6acbfae582d46b5d249b2e31fc6eed7071b1b8388648a60d090029d356b462\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pybase64, overrides, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.16 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create the source directory if it doesn't exist\n",
        "if not os.path.exists('./source'):\n",
        "    os.makedirs('./source')\n",
        "\n",
        "# Create a dummy text file in the source directory\n",
        "with open('./source/dummy_doc.txt', 'w') as f:\n",
        "    f.write('This is a dummy document for testing.')"
      ],
      "metadata": {
        "id": "gllSBhovoTLX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "### Reading the txt files from source directory\n",
        "\n",
        "loader = DirectoryLoader('./source', glob=\"./*.txt\", loader_cls=TextLoader)\n",
        "docs = loader.load()\n",
        "\n",
        "### Creating Chunks using RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=50,\n",
        "    chunk_overlap=10,\n",
        "    length_function=len\n",
        ")\n",
        "new_docs = text_splitter.split_documents(documents=docs)\n",
        "doc_strings = [doc.page_content for doc in new_docs]\n",
        "\n",
        "###  BGE Embddings\n",
        "\n",
        "'''from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs,\n",
        ")\n",
        "'''\n",
        "\n",
        "### Creating Retriever using Vector DB\n",
        "\n",
        "db = Chroma.from_documents(new_docs, embeddings)\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "dtsayUH0ih20"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n"
      ],
      "metadata": {
        "id": "FiBFP1gnihzp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_chain = (\n",
        "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    )"
      ],
      "metadata": {
        "id": "XRU-Qk2DihxL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question =\"Pakistan?\""
      ],
      "metadata": {
        "id": "ypp9CCKKihuV"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_chain.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v6aDl_ryihr_",
        "outputId": "e718205c-7ea4-4a24-e53a-8f47da47663a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The provided context does not contain any information about Pakistan. Therefore, I cannot answer the question.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "result = retrieval_chain.invoke(question)\n",
        "\n",
        "print('Time taken:',time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdni9SSSihpq",
        "outputId": "81cc1965-7d26-45e2-f5a2-7f40df267510"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 1.0051040649414062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "batch_output = retrieval_chain.batch([\n",
        "                        \"what is llama3?\",\n",
        "                        \"can you highlight 3 main properties?\"\n",
        "                       ])\n",
        "\n",
        "print('Time taken:',time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gMip1kPihm2",
        "outputId": "875d97ce-9ea0-49cc-8ddb-35931c1393b1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 2.474843740463257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G6ubIZ4ihiR",
        "outputId": "5a46b527-d1e5-4b19-8d50-c845424f5ca1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Based on the provided context, I cannot answer the question \"what is llama3?\". The document only contains the text \"This is a dummy document for testing.\" and does not mention llama3.',\n",
              " 'Based on the provided context, I cannot highlight 3 main properties. The document only contains a single sentence: \"This is a dummy document for testing.\" There are no properties mentioned.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in the following language: {language}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n"
      ],
      "metadata": {
        "id": "h5ykxzWrihdn"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03abf681"
      },
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "retrieval_chain = (\n",
        "    RunnableParallel({\"context\": itemgetter('question') | retriever,\n",
        "                       \"question\": itemgetter('question'),\n",
        "                       \"language\": itemgetter('language')\n",
        "                       })\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    )"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### itemgetter only works with dictionaries , input has to be a dict\n",
        "\n",
        "response = retrieval_chain.invoke({'question': \"what is llama3?\",\n",
        "                        'language': \"Spnish\"})\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecR-awYSqG3F",
        "outputId": "4c420da9-2be3-43fe-bed4-e4e03a34ad87"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No se puede determinar qué es llama3 basándome en la información proporcionada. El documento solo dice que es un documento de prueba.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = 'Hi! I am learning {skill}. Can you suggest me top 5 things to learn?\\n'\n",
        "\n",
        "prompt = PromptTemplate.from_template(template=template)\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "UAsl2-hHqGna"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in chain.stream({'skill':'Big Data'}):\n",
        "    print(s.content,end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJrobIsFqGjk",
        "outputId": "eee4107c-37ab-4da5-eed9-07cb3d56bc2f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here are my top 5 suggestions for what to learn when starting with Big Data, designed to give you a solid foundation:\n",
            "\n",
            "1.  **Fundamentals of Data and Databases (SQL & NoSQL):**\n",
            "\n",
            "    *   **Why it's important:** You need to understand how data is structured, stored, and retrieved. This is the bedrock of any big data project.  Even with advanced tools, you'll often need to interact with data through databases.\n",
            "    *   **What to learn:**\n",
            "        *   **SQL (Structured Query Language):** Learn the basics of querying relational databases (e.g., MySQL, PostgreSQL, SQL Server). Focus on `SELECT`, `FROM`, `WHERE`, `JOIN`, `GROUP BY`, and `ORDER BY` clauses.  Understand data types and normalization.\n",
            "        *   **NoSQL Databases:** Get a conceptual understanding of NoSQL databases like MongoDB (document-oriented), Cassandra (column-family), and Redis (key-value).  Understand when and why you'd choose a NoSQL database over SQL.  Learn basic CRUD (Create, Read, Update, Delete) operations for at least one NoSQL database (MongoDB is a good starting point).\n",
            "    *   **Resources:**\n",
            "        *   **SQL:**  Codecademy, Khan Academy, SQLZoo, W3Schools, DataCamp (paid).\n",
            "        *   **NoSQL:** MongoDB University, official documentation for specific NoSQL databases.\n",
            "\n",
            "2.  **Hadoop Ecosystem (Core Components):**\n",
            "\n",
            "    *   **Why it's important:** Hadoop is still a foundational technology in Big Data. Understanding its core components will give you a solid base for understanding other related tools.\n",
            "    *   **What to learn:**\n",
            "        *   **HDFS (Hadoop Distributed File System):**  How data is stored in a distributed and fault-tolerant manner. Understand the concepts of blocks, data replication, and Namenode/Datanode architecture.\n",
            "        *   **MapReduce:**  The original programming model for processing large datasets in parallel.  Understand the map and reduce phases.  While you might not write MapReduce jobs directly as often now, the concepts are still important.\n",
            "        *   **YARN (Yet Another Resource Negotiator):** The resource management layer that manages cluster resources.\n",
            "    *   **Resources:**\n",
            "        *   **Hadoop Documentation:** The official Apache Hadoop documentation is comprehensive.\n",
            "        *   **Cloudera's Hadoop Tutorial:** A good introductory tutorial.\n",
            "        *   **Books:** \"Hadoop: The Definitive Guide\" by Tom White.\n",
            "\n",
            "3.  **Data Processing Frameworks (Spark):**\n",
            "\n",
            "    *   **Why it's important:**  Spark is a powerful and widely used framework for processing large datasets in memory, making it significantly faster than traditional MapReduce.\n",
            "    *   **What to learn:**\n",
            "        *   **Spark Core:**  RDDs (Resilient Distributed Datasets), transformations, and actions.  Learn how to read data from different sources (HDFS, local files, etc.).\n",
            "        *   **Spark SQL:**  Querying structured data within Spark using SQL-like syntax.\n",
            "        *   **Spark Streaming (Optional, but recommended):**  Basic concepts of real-time data processing.\n",
            "    *   **Resources:**\n",
            "        *   **Spark Documentation:** The official Apache Spark documentation is excellent.\n",
            "        *   **Spark Tutorials:** Databricks (the company behind Spark) offers excellent tutorials and a free community edition to play around.\n",
            "        *   **Online Courses:** Udemy, Coursera, edX (search for \"Spark\").\n",
            "\n",
            "4.  **Data Warehousing Concepts & Tools (Data Lakes and Data Warehouses):**\n",
            "\n",
            "    *   **Why it's important:** You need to understand how to store and manage large datasets for analytical purposes.\n",
            "    *   **What to learn:**\n",
            "        *   **Data Warehousing Principles:** Understand concepts like star schemas, fact tables, dimension tables, ETL (Extract, Transform, Load) processes, and data modeling.\n",
            "        *   **Data Lakes:**  The concept of a data lake, its advantages, and disadvantages.  How it differs from a data warehouse.\n",
            "        *   **Data Warehouse Tools (choose one to start):**\n",
            "            *   **Cloud-based options:**  AWS Redshift, Google BigQuery, Azure Synapse Analytics.  These are managed services, which makes them easier to set up and maintain.\n",
            "            *   **Open-source options (more complex):**  Apache Hive (built on Hadoop), Apache Impala.\n",
            "    *   **Resources:**\n",
            "        *   **Books:** \"Data Warehousing Fundamentals\" by Paulraj Ponniah, \"Building the Data Warehouse\" by W.H. Inmon.\n",
            "        *   **Cloud Provider Documentation:**  AWS, Google Cloud, and Azure have excellent documentation and tutorials for their data warehousing services.\n",
            "        *   **Online Courses:**  Look for courses on data warehousing fundamentals and specific tools.\n",
            "\n",
            "5.  **Programming Language for Data Analysis (Python):**\n",
            "\n",
            "    *   **Why it's important:**  You'll need a programming language to work with data, build data pipelines, and perform data analysis.  Python is the most popular language in the Big Data space due to its extensive libraries.\n",
            "    *   **What to learn:**\n",
            "        *   **Python Fundamentals:**  Basic syntax, data types, control flow (loops, conditionals), functions, and object-oriented programming.\n",
            "        *   **Key Data Science Libraries:**\n",
            "            *   **Pandas:**  For data manipulation and analysis (working with DataFrames).\n",
            "            *   **NumPy:**  For numerical computations.\n",
            "            *   **Matplotlib/Seaborn:**  For data visualization.\n",
            "            *   **Scikit-learn:**  For machine learning algorithms (optional at the beginning, but very useful later).\n",
            "    *   **Resources:**\n",
            "        *   **Python Documentation:** The official Python documentation is excellent.\n",
            "        *   **Online Courses:**  Codecademy, DataCamp, Coursera, edX, Udemy (search for \"Python for Data Science\").\n",
            "        *   **Books:** \"Python for Data Analysis\" by Wes McKinney (the creator of Pandas).\n",
            "\n",
            "**How to Approach Learning:**\n",
            "\n",
            "*   **Start with the Fundamentals:**  Don't jump straight into advanced tools. Build a solid foundation in SQL, data storage, and basic programming.\n",
            "*   **Hands-on Practice:**  The most important thing is to *do*.  Download the tools, work through tutorials, and build small projects.  Use sample datasets.\n",
            "*   **Focus on One Thing at a Time:**  Don't try to learn everything at once. Pick a topic (e.g., SQL), learn the basics, practice, and then move on.\n",
            "*   **Build Small Projects:**  Create small projects to apply what you're learning.  For example:\n",
            "    *   Load a CSV file into a database and query it using SQL.\n",
            "    *   Use Spark to process a sample dataset (e.g., a log file).\n",
            "    *   Build a simple data pipeline to transform and load data.\n",
            "*   **Stay Curious:**  Big Data is a constantly evolving field. Be prepared to learn new technologies and techniques as they emerge.\n",
            "\n",
            "Good luck with your Big Data journey! Let me know if you have any other questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "\n",
        "@tool\n",
        "def multiply(first_number: int, second_number: int):\n",
        "    \"\"\"Multiplies two numbers together.\"\"\"\n",
        "    return first_number * second_number\n",
        "\n",
        "model_with_tools = llm.bind(tools=[convert_to_openai_tool(multiply)])"
      ],
      "metadata": {
        "id": "LNX861ZtqSiX"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model_with_tools.invoke('What is 35 * 46?')"
      ],
      "metadata": {
        "id": "f1i7UdZlqScL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54ky87j7qSWf",
        "outputId": "47abcb30-cfe4-4c1e-946b-084909c2c122"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"second_number\": 46.0, \"first_number\": 35.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite-001', 'safety_ratings': []}, id='run--4cafb001-a64a-4e38-9605-d10db84eb1f9-0', tool_calls=[{'name': 'multiply', 'args': {'second_number': 46.0, 'first_number': 35.0}, 'id': 'f0a500f3-24a4-467e-92b6-7e4700d2c2f1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 32, 'output_tokens': 9, 'total_tokens': 41, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About the Author\n",
        "\n",
        "<div style=\"background-color: #f8f9fa; border-left: 5px solid #28a745; padding: 20px; margin-bottom: 20px; border-radius: 5px;\">\n",
        "  <h2 style=\"color: #28a745; margin-top: 0; font-family: 'Poppins', sans-serif;\">Muhammad Atif Latif</h2>\n",
        "  <p style=\"font-size: 16px; color: #495057;\">Data Scientist & Machine Learning Engineer</p>\n",
        "  \n",
        "  <p style=\"font-size: 15px; color: #6c757d; margin-top: 15px;\">\n",
        "    Passionate about building AI solutions that solve real-world problems. Specialized in machine learning,\n",
        "    deep learning, and data analytics with experience implementing production-ready models.\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "## Connect With Me\n",
        "\n",
        "<div style=\"display: flex; flex-wrap: wrap; gap: 10px; margin-top: 15px;\">\n",
        "  <a href=\"https://github.com/m-Atif-Latif\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/GitHub-Follow-212121?style=for-the-badge&logo=github\" alt=\"GitHub\">\n",
        "  </a>\n",
        "  <a href=\"https://www.kaggle.com/matiflatif\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/Kaggle-Profile-20BEFF?style=for-the-badge&logo=kaggle\" alt=\"Kaggle\">\n",
        "  </a>\n",
        "  <a href=\"https://www.linkedin.com/in/muhammad-atif-latif-13a171318\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/LinkedIn-Connect-0077B5?style=for-the-badge&logo=linkedin\" alt=\"LinkedIn\">\n",
        "  </a>\n",
        "  <a href=\"https://x.com/mianatif5867\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/Twitter-Follow-1DA1F2?style=for-the-badge&logo=twitter\" alt=\"Twitter\">\n",
        "  </a>\n",
        "  <a href=\"https://www.instagram.com/its_atif_ai/\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/Instagram-Follow-E4405F?style=for-the-badge&logo=instagram\" alt=\"Instagram\">\n",
        "  </a>\n",
        "  <a href=\"mailto:muhammadatiflatif67@gmail.com\">\n",
        "    <img src=\"https://img.shields.io/badge/Email-Contact-D14836?style=for-the-badge&logo=gmail\" alt=\"Email\">\n",
        "  </a>\n",
        "</div>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6JBRKNZcqdKg"
      }
    }
  ]
}