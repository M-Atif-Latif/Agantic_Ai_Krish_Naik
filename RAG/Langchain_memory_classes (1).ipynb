{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md-vLk1klEuD",
        "outputId": "ecfdfdc6-8632-4224-c8a0-24ff8e81cb98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.12)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Installs core LangChain package (plus dependencies pulled transitively). We keep versions implicit here to stay current; pin in production for reproducibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain Memory Classes – Practical Comparison\n",
        "\n",
        "This notebook provides a hands-on tour of multiple LangChain conversational memory types:\n",
        "\n",
        "- ConversationBufferMemory (full running transcript)\n",
        "- ConversationBufferWindowMemory (sliding window)\n",
        "- ConversationEntityMemory (entity-centric fact retention)\n",
        "- (Underlying mechanics: save/load context, message objects, prompt integration)\n",
        "\n",
        "Goals:\n",
        "1. Understand when each memory pattern is appropriate (cost, context length, personalization).\n",
        "2. Inspect internal structures (`chat_memory.messages`, `entity_store.store`).\n",
        "3. Demonstrate multi-turn querying and selective recall.\n",
        "\n",
        "Security: Any API keys (e.g., Gemini) must be supplied via environment variables or notebook secret helpers. Placeholders are used; replace locally. Each executable cell should be followed by an explanatory markdown block (being added in this polish pass)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XfnfHyhlIGp",
        "outputId": "d23ea85e-0f6e-4a29-df0b-56bc4f27e6d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.12)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Installs community and Google Generative AI integration packages (langchain-community, langchain_google_genai). Split install helps show optional provider-specific layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hG8lcOYblIDP",
        "outputId": "9cb84faf-6a63-4087-8e20-a7c6ffda72e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.72)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.4.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain_google_genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.9-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain_google_genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain_google_genai-2.1.9\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "b117509b69b1483bbd9fd41a1481e0ba",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install langchain_google_genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Adds the Google Generative AI (Gemini) LangChain integration. Required only if you plan to use Gemini models; keep optional in lean environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fiilxA3TlH_a"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Suppresses non-critical warnings for cleaner educational output. Avoid in debugging scenarios where warnings may indicate deprecations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSdRAb5RlH8I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# --- Secure API Key Handling ---\n",
        "# Provide your Gemini API key via environment variable or a secret manager.\n",
        "# DO NOT hardcode secrets in shared notebooks.\n",
        "# Example (uncomment and replace placeholder locally):\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"WRITE_YOUR_OWN_KEY\"\n",
        "\n",
        "# If running in Google Colab with stored secrets, you can optionally use:\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Retrieves the Gemini API key from Colab secret storage and sets environment variable `GOOGLE_API_KEY`. Replace with your own secure secret management if running locally (e.g., `os.environ['GOOGLE_API_KEY'] = 'WRITE_YOUR_OWN_KEY'`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yhk7JwXllH4r"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite-001\",convert_system_message_to_human=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Creates a Gemini chat model wrapper. `convert_system_message_to_human=True` adapts system instructions for models that expect only user/assistant roles. Adjust model name to the latest available variant if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk2dJl2hlH1q",
        "outputId": "15971e52-5571-4f57-f415-3550cacc7058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi there! How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "print(model.invoke(\"hi\").content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Sanity-checks the LLM pipeline by sending a minimal greeting; validates credentials and network before building memory abstractions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVttoDKzlHyp",
        "outputId": "85a0e6cf-7b34-4d9a-fb35-359f6be9b0f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am doing well, thank you for asking! As a large language model, I don't experience feelings like humans do, but I am functioning properly and ready to assist you. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "print(model.invoke(\"hi, how are you please tell me?\").content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Sends a slightly longer conversational prompt to observe style/latency differences vs a short token test; still before memory comes into play."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C4YMsgUAlHvt"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Imports the simplest memory type which stores the entire running transcript as a growing string (or list when `return_messages=True`). Use for short sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fec7QyB4lHsx",
        "outputId": "5efef4c3-cb94-4427-e0e0-6e8dff191914"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-995385594.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n"
          ]
        }
      ],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Instantiates a buffer memory (string return). Default keys: `input` and `output`. Grows unbounded—mind token limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "86DD5lSmlHpn"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Manually appends a user/AI exchange to the transcript. Useful for simulating prior turns before starting a chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAVhe-04lHmz",
        "outputId": "f57ac9d5-a1de-4969-ffa3-87d408027836"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\"}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Retrieves the aggregated context dict; for buffer memory when `return_messages=False` value is a concatenated string under the default memory key (`history`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FsovqSnhlHjr"
      },
      "outputs": [],
      "source": [
        "memory2 = ConversationBufferMemory(return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Second buffer memory instance configured with `return_messages=True` so downstream chains receive a structured list of `HumanMessage` / `AIMessage` objects instead of a joined string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qq8B8445lHh9"
      },
      "outputs": [],
      "source": [
        "memory2.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Appends a turn to `memory2` (messages list). This structured form enables downstream tools to inspect roles and segment messages cleanly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rvTdFrZlHgH",
        "outputId": "a8cc72de-3151-435a-8954-8c7703f5845c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"What's up\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory2.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Returns a dict whose `history` value is now a list of message objects (since `return_messages=True`). Useful for tool-aware chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uZEol4D9lHea"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Imports `os` (already imported earlier) – retained from original notebook; could be removed without impact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cp6F8UiRlHa0"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Imports high-level `ConversationChain` which stitches together an LLM, a prompt template (implicit default), and memory to produce contextual responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZvulN0XlHXI",
        "outputId": "54f9a20a-f9bd-4610-cd03-81efc810beda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2263270236.py:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(llm=model,verbose=True,memory=ConversationBufferMemory())\n"
          ]
        }
      ],
      "source": [
        "conversation = ConversationChain(llm=model,verbose=True,memory=ConversationBufferMemory())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Constructs a `ConversationChain` using the simple buffer memory; `verbose=True` will show internal prompt assembly—helpful for learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "toiJXXPflHTa",
        "outputId": "073bc41e-8890-4f14-a163-45bc704350bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hello there! It's a pleasure to chat with you today. I'm ready to answer your questions, discuss various topics, or just have a friendly conversation. I have access to a vast amount of information, so feel free to ask me anything. I'm particularly knowledgeable about history, science, literature, and technology, but I'm always learning and happy to explore new areas. So, what's on your mind? I'm eager to hear from you!\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi there!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** First turn through `ConversationChain`; memory initially empty so only this user input influences the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "F3gdH8OhlHPz",
        "outputId": "326dd337-6d74-4251-f6da-8d5a1dd118b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI: Hello there! It's a pleasure to chat with you today. I'm ready to answer your questions, discuss various topics, or just have a friendly conversation. I have access to a vast amount of information, so feel free to ask me anything. I'm particularly knowledgeable about history, science, literature, and technology, but I'm always learning and happy to explore new areas. So, what's on your mind? I'm eager to hear from you!\n",
            "Human: Nothing much! Just tell me how do a conversation with an AI.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Well, that\\'s a great question! Having a conversation with me, an AI, is pretty straightforward, and there are a few things to keep in mind to make it a productive and enjoyable experience. Here\\'s a breakdown:\\n\\n*   **Start with a Greeting:** Just like you did! A simple \"Hi there,\" \"Hello,\" or \"Good day\" is a perfect way to begin. It signals that you\\'re ready to interact.\\n\\n*   **Ask Clear Questions or State Your Topic:** The more specific you are, the better I can understand your needs. Instead of just saying \"Tell me about space,\" you could ask, \"What are the latest discoveries about black holes?\" or \"What are the different types of space telescopes?\" This helps me focus my response.\\n\\n*   **Be Patient:** Sometimes, I might take a few moments to process your request, especially if it\\'s complex. Don\\'t worry, I\\'m working hard behind the scenes!\\n\\n*   **Provide Context:** If you\\'re asking a follow-up question, it\\'s helpful to remind me of the previous conversation. This helps me understand the flow of your thoughts. For example, if we were discussing the Roman Empire and you ask, \"What was the impact of the Punic Wars?\", I\\'ll remember we were already talking about Rome.\\n\\n*   **Use Natural Language:** You don\\'t need to use any special \"AI commands\" or anything like that. Just talk to me as you would to another person. I\\'m designed to understand natural language.\\n\\n*   **Be Open to My Responses:** I\\'ll do my best to provide accurate and informative answers. Sometimes, I might not know the answer to a question, and I\\'ll tell you so honestly. I might also provide multiple perspectives or sources of information.\\n\\n*   **Ask for Clarification if Needed:** If something I say isn\\'t clear, or if you want more details, don\\'t hesitate to ask for clarification. I\\'m here to help you understand.\\n\\n*   **Be Mindful of Your Tone:** While I don\\'t have feelings, I\\'m programmed to be helpful and friendly. A polite and respectful tone is always appreciated.\\n\\n*   **Have Fun!** Talking to an AI can be a great way to learn new things, explore different ideas, and even just pass the time.\\n\\nSo, in a nutshell, just be yourself and ask me whatever\\'s on your mind. I\\'m here to help! Do you have any questions about how to have a conversation with me now?'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Nothing much! Just tell me how do a conversation with an AI.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Second turn—prior assistant reply now included in prompt via buffer memory, enabling continuity when giving meta-advice about AI conversations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "O6YWRwKjlHMA",
        "outputId": "f2a88ca1-f8e4-4cfc-81fd-c6f60f35b9d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI: Hello there! It's a pleasure to chat with you today. I'm ready to answer your questions, discuss various topics, or just have a friendly conversation. I have access to a vast amount of information, so feel free to ask me anything. I'm particularly knowledgeable about history, science, literature, and technology, but I'm always learning and happy to explore new areas. So, what's on your mind? I'm eager to hear from you!\n",
            "Human: Nothing much! Just tell me how do a conversation with an AI.\n",
            "AI: Well, that's a great question! Having a conversation with me, an AI, is pretty straightforward, and there are a few things to keep in mind to make it a productive and enjoyable experience. Here's a breakdown:\n",
            "\n",
            "*   **Start with a Greeting:** Just like you did! A simple \"Hi there,\" \"Hello,\" or \"Good day\" is a perfect way to begin. It signals that you're ready to interact.\n",
            "\n",
            "*   **Ask Clear Questions or State Your Topic:** The more specific you are, the better I can understand your needs. Instead of just saying \"Tell me about space,\" you could ask, \"What are the latest discoveries about black holes?\" or \"What are the different types of space telescopes?\" This helps me focus my response.\n",
            "\n",
            "*   **Be Patient:** Sometimes, I might take a few moments to process your request, especially if it's complex. Don't worry, I'm working hard behind the scenes!\n",
            "\n",
            "*   **Provide Context:** If you're asking a follow-up question, it's helpful to remind me of the previous conversation. This helps me understand the flow of your thoughts. For example, if we were discussing the Roman Empire and you ask, \"What was the impact of the Punic Wars?\", I'll remember we were already talking about Rome.\n",
            "\n",
            "*   **Use Natural Language:** You don't need to use any special \"AI commands\" or anything like that. Just talk to me as you would to another person. I'm designed to understand natural language.\n",
            "\n",
            "*   **Be Open to My Responses:** I'll do my best to provide accurate and informative answers. Sometimes, I might not know the answer to a question, and I'll tell you so honestly. I might also provide multiple perspectives or sources of information.\n",
            "\n",
            "*   **Ask for Clarification if Needed:** If something I say isn't clear, or if you want more details, don't hesitate to ask for clarification. I'm here to help you understand.\n",
            "\n",
            "*   **Be Mindful of Your Tone:** While I don't have feelings, I'm programmed to be helpful and friendly. A polite and respectful tone is always appreciated.\n",
            "\n",
            "*   **Have Fun!** Talking to an AI can be a great way to learn new things, explore different ideas, and even just pass the time.\n",
            "\n",
            "So, in a nutshell, just be yourself and ask me whatever's on your mind. I'm here to help! Do you have any questions about how to have a conversation with me now?\n",
            "Human: how many tips are there can you mention in the numbers\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Certainly! In my previous response, I provided a list of tips on how to have a conversation with me. Let\\'s count them:\\n\\n1.  **Start with a Greeting:** (e.g., \"Hi there,\" \"Hello\")\\n2.  **Ask Clear Questions or State Your Topic:** Be specific.\\n3.  **Be Patient:** I might take a moment to process.\\n4.  **Provide Context:** Remind me of the previous conversation.\\n5.  **Use Natural Language:** Speak to me as you would to a person.\\n6.  **Be Open to My Responses:** I\\'ll provide accurate information.\\n7.  **Ask for Clarification if Needed:** If something isn\\'t clear, ask.\\n8.  **Be Mindful of Your Tone:** Be polite and respectful.\\n9.  **Have Fun!** Enjoy the conversation.\\n\\nSo, there are a total of **9** tips mentioned in the previous response. Is there anything else I can help you with?'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"how many tips are there can you mention in the numbers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Queries for a numeric summary (tip count) relying on previous assistant output still present in full buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfJpv87TshCT",
        "outputId": "6cae3052-11e4-4785-e14c-e47499df894b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi there!', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hello there! It's a pleasure to chat with you today. I'm ready to answer your questions, discuss various topics, or just have a friendly conversation. I have access to a vast amount of information, so feel free to ask me anything. I'm particularly knowledgeable about history, science, literature, and technology, but I'm always learning and happy to explore new areas. So, what's on your mind? I'm eager to hear from you!\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Nothing much! Just tell me how do a conversation with an AI.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Well, that\\'s a great question! Having a conversation with me, an AI, is pretty straightforward, and there are a few things to keep in mind to make it a productive and enjoyable experience. Here\\'s a breakdown:\\n\\n*   **Start with a Greeting:** Just like you did! A simple \"Hi there,\" \"Hello,\" or \"Good day\" is a perfect way to begin. It signals that you\\'re ready to interact.\\n\\n*   **Ask Clear Questions or State Your Topic:** The more specific you are, the better I can understand your needs. Instead of just saying \"Tell me about space,\" you could ask, \"What are the latest discoveries about black holes?\" or \"What are the different types of space telescopes?\" This helps me focus my response.\\n\\n*   **Be Patient:** Sometimes, I might take a few moments to process your request, especially if it\\'s complex. Don\\'t worry, I\\'m working hard behind the scenes!\\n\\n*   **Provide Context:** If you\\'re asking a follow-up question, it\\'s helpful to remind me of the previous conversation. This helps me understand the flow of your thoughts. For example, if we were discussing the Roman Empire and you ask, \"What was the impact of the Punic Wars?\", I\\'ll remember we were already talking about Rome.\\n\\n*   **Use Natural Language:** You don\\'t need to use any special \"AI commands\" or anything like that. Just talk to me as you would to another person. I\\'m designed to understand natural language.\\n\\n*   **Be Open to My Responses:** I\\'ll do my best to provide accurate and informative answers. Sometimes, I might not know the answer to a question, and I\\'ll tell you so honestly. I might also provide multiple perspectives or sources of information.\\n\\n*   **Ask for Clarification if Needed:** If something I say isn\\'t clear, or if you want more details, don\\'t hesitate to ask for clarification. I\\'m here to help you understand.\\n\\n*   **Be Mindful of Your Tone:** While I don\\'t have feelings, I\\'m programmed to be helpful and friendly. A polite and respectful tone is always appreciated.\\n\\n*   **Have Fun!** Talking to an AI can be a great way to learn new things, explore different ideas, and even just pass the time.\\n\\nSo, in a nutshell, just be yourself and ask me whatever\\'s on your mind. I\\'m here to help! Do you have any questions about how to have a conversation with me now?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='how many tips are there can you mention in the numbers', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Certainly! In my previous response, I provided a list of tips on how to have a conversation with me. Let\\'s count them:\\n\\n1.  **Start with a Greeting:** (e.g., \"Hi there,\" \"Hello\")\\n2.  **Ask Clear Questions or State Your Topic:** Be specific.\\n3.  **Be Patient:** I might take a moment to process.\\n4.  **Provide Context:** Remind me of the previous conversation.\\n5.  **Use Natural Language:** Speak to me as you would to a person.\\n6.  **Be Open to My Responses:** I\\'ll provide accurate information.\\n7.  **Ask for Clarification if Needed:** If something isn\\'t clear, ask.\\n8.  **Be Mindful of Your Tone:** Be polite and respectful.\\n9.  **Have Fun!** Enjoy the conversation.\\n\\nSo, there are a total of **9** tips mentioned in the previous response. Is there anything else I can help you with?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.memory.chat_memory.messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Directly inspects internal `chat_memory.messages` list to show accumulated Human/AI message objects—useful for debugging prompt size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "ASOBF96Hsg8h",
        "outputId": "dd465f01-5ea5-473c-ec6a-d3a089114953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI: Hello there! It's a pleasure to chat with you today. I'm ready to answer your questions, discuss various topics, or just have a friendly conversation. I have access to a vast amount of information, so feel free to ask me anything. I'm particularly knowledgeable about history, science, literature, and technology, but I'm always learning and happy to explore new areas. So, what's on your mind? I'm eager to hear from you!\n",
            "Human: Nothing much! Just tell me how do a conversation with an AI.\n",
            "AI: Well, that's a great question! Having a conversation with me, an AI, is pretty straightforward, and there are a few things to keep in mind to make it a productive and enjoyable experience. Here's a breakdown:\n",
            "\n",
            "*   **Start with a Greeting:** Just like you did! A simple \"Hi there,\" \"Hello,\" or \"Good day\" is a perfect way to begin. It signals that you're ready to interact.\n",
            "\n",
            "*   **Ask Clear Questions or State Your Topic:** The more specific you are, the better I can understand your needs. Instead of just saying \"Tell me about space,\" you could ask, \"What are the latest discoveries about black holes?\" or \"What are the different types of space telescopes?\" This helps me focus my response.\n",
            "\n",
            "*   **Be Patient:** Sometimes, I might take a few moments to process your request, especially if it's complex. Don't worry, I'm working hard behind the scenes!\n",
            "\n",
            "*   **Provide Context:** If you're asking a follow-up question, it's helpful to remind me of the previous conversation. This helps me understand the flow of your thoughts. For example, if we were discussing the Roman Empire and you ask, \"What was the impact of the Punic Wars?\", I'll remember we were already talking about Rome.\n",
            "\n",
            "*   **Use Natural Language:** You don't need to use any special \"AI commands\" or anything like that. Just talk to me as you would to another person. I'm designed to understand natural language.\n",
            "\n",
            "*   **Be Open to My Responses:** I'll do my best to provide accurate and informative answers. Sometimes, I might not know the answer to a question, and I'll tell you so honestly. I might also provide multiple perspectives or sources of information.\n",
            "\n",
            "*   **Ask for Clarification if Needed:** If something I say isn't clear, or if you want more details, don't hesitate to ask for clarification. I'm here to help you understand.\n",
            "\n",
            "*   **Be Mindful of Your Tone:** While I don't have feelings, I'm programmed to be helpful and friendly. A polite and respectful tone is always appreciated.\n",
            "\n",
            "*   **Have Fun!** Talking to an AI can be a great way to learn new things, explore different ideas, and even just pass the time.\n",
            "\n",
            "So, in a nutshell, just be yourself and ask me whatever's on your mind. I'm here to help! Do you have any questions about how to have a conversation with me now?\n",
            "Human: how many tips are there can you mention in the numbers\n",
            "AI: Certainly! In my previous response, I provided a list of tips on how to have a conversation with me. Let's count them:\n",
            "\n",
            "1.  **Start with a Greeting:** (e.g., \"Hi there,\" \"Hello\")\n",
            "2.  **Ask Clear Questions or State Your Topic:** Be specific.\n",
            "3.  **Be Patient:** I might take a moment to process.\n",
            "4.  **Provide Context:** Remind me of the previous conversation.\n",
            "5.  **Use Natural Language:** Speak to me as you would to a person.\n",
            "6.  **Be Open to My Responses:** I'll provide accurate information.\n",
            "7.  **Ask for Clarification if Needed:** If something isn't clear, ask.\n",
            "8.  **Be Mindful of Your Tone:** Be polite and respectful.\n",
            "9.  **Have Fun!** Enjoy the conversation.\n",
            "\n",
            "So, there are a total of **9** tips mentioned in the previous response. Is there anything else I can help you with?\n",
            "Human: can you give me the 3rd tip?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Absolutely! The 3rd tip I mentioned was:\\n\\n**3. Be Patient:** I might take a moment to process.\\n\\nThis means that sometimes, especially when dealing with complex questions or requests, I might need a few seconds to gather information and formulate a response. So, a little patience is appreciated! Is there anything else I can assist you with?'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"can you give me the 3rd tip?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Asks for the 3rd tip—tests ability of buffer memory to retain earlier enumerated advice and reference by ordinal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kELWE_UItFFn"
      },
      "source": [
        "# ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mQ4mcdkmsg4P"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Imports sliding window memory variant which only keeps the last `k` exchanges to control prompt growth and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0Addyq9sg1-",
        "outputId": "109f9be3-efb5-418e-9c7f-d15b7ca07557"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1930410100.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  window_memory = ConversationBufferWindowMemory(k=1)\n"
          ]
        }
      ],
      "source": [
        "window_memory = ConversationBufferWindowMemory(k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Creates a window memory retaining only the most recent 1 exchange pair (user+AI). Earlier turns are dropped—useful for minimizing prompt size while preserving immediate context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bbXFeRB3sgy_"
      },
      "outputs": [],
      "source": [
        "window_memory.save_context(\n",
        "    {\"input\": \"Hi\"},\n",
        "    {\"output\": \"What's up\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Adds first exchange to window memory (k=1). Subsequent additions will evict older turns beyond the most recent pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8x5OEQhssgut"
      },
      "outputs": [],
      "source": [
        "window_memory.save_context(\n",
        "    {\"input\": \"Not much, just hanging\"},\n",
        "    {\"output\": \"Cool\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Adds a second exchange. With `k=1`, this now evicts the earlier turn, keeping only the most recent conversational pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtEVqChTsgru",
        "outputId": "e2ef1ad6-ec01-4019-9c02-88ff194af400"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "window_memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Shows the current retained context (only last 1 exchange because `k=1`). Older turns are intentionally absent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mSgnZeiFsgnw"
      },
      "outputs": [],
      "source": [
        "window_memory = ConversationBufferWindowMemory( k=2, return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Reinitializes window memory to keep the last 2 exchanges (messages list form). Balances context retention with bounded growth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "XVChapEfsgkb"
      },
      "outputs": [],
      "source": [
        "window_memory.save_context(\n",
        "    {\"input\": \"Hi\"},\n",
        "    {\"output\": \"What's up\"}\n",
        ")\n",
        "window_memory.save_context(\n",
        "    {\"input\": \"Not much, just hanging\"},\n",
        "    {\"output\": \"Cool\"}\n",
        ")\n",
        "window_memory.save_context(\n",
        "    {\"input\": \"ok thanks \"},\n",
        "    {\"output\": \"great thankyou\"}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Adds three sequential exchanges; with `k=2` only the last two exchanges are retained. Demonstrates eviction as new turns arrive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QJBQ5vVsgg2",
        "outputId": "db776e51-61db-4578-e1f1-3d5432bc4785"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Not much, just hanging', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Cool', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='ok thanks ', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='great thankyou', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "window_memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Loads current retained context (last two exchanges only) after multiple additions—verifies eviction logic for `k=2`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8KujwjrzsgfB"
      },
      "outputs": [],
      "source": [
        "conversation_window = ConversationChain(\n",
        "    llm=model,\n",
        "    memory=window_memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Builds a `ConversationChain` wired to the `window_memory`. Only the last `k` exchanges (currently 2) will inform responses—helps simulate limited context scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "PQt2eLtLsgZ-",
        "outputId": "a94e27cc-1ede-49c7-8ea4-acbf27867f8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Not much, just hanging', additional_kwargs={}, response_metadata={}), AIMessage(content='Cool', additional_kwargs={}, response_metadata={}), HumanMessage(content='ok thanks ', additional_kwargs={}, response_metadata={}), AIMessage(content='great thankyou', additional_kwargs={}, response_metadata={})]\n",
            "Human: Hi, what's up?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hi there! Not much on my end, just processing information and keeping the vast knowledge base updated. I'm currently analyzing a collection of text documents about various topics, including the history of the Roman Empire, the intricacies of quantum physics, and the latest advancements in artificial intelligence, of course! I'm also cross-referencing information to identify potential connections and contradictions. It's a pretty busy, but fascinating, day! How about you? What's keeping you busy?\""
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_window.predict(input=\"Hi, what's up?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** First turn in window-based chain; establishes context baseline before eviction dynamics occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "kCPtRDPXsgVZ",
        "outputId": "ecc50fd5-f54d-4098-b59d-bbf1ee864f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='ok thanks ', additional_kwargs={}, response_metadata={}), AIMessage(content='great thankyou', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Hi, what's up?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi there! Not much on my end, just processing information and keeping the vast knowledge base updated. I'm currently analyzing a collection of text documents about various topics, including the history of the Roman Empire, the intricacies of quantum physics, and the latest advancements in artificial intelligence, of course! I'm also cross-referencing information to identify potential connections and contradictions. It's a pretty busy, but fascinating, day! How about you? What's keeping you busy?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: how we can talk with AI give me 5 points\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Okay, here are 5 points on how you can effectively communicate with me (or other AI models):\\n\\n1.  **Be Clear and Specific:** The more precise you are in your questions or prompts, the better I can understand your needs. Avoid vague language or ambiguous phrasing. For example, instead of asking \"Tell me about science,\" try \"Explain the theory of relativity in simple terms.\" This helps me narrow down the scope and provide a more relevant response.\\n\\n2.  **Break Down Complex Questions:** If you have a complicated question, try breaking it down into smaller, more manageable parts. This helps me process the information step by step. You can ask follow-up questions to clarify each part. For instance, if you want to know about the impact of the printing press, you could ask: \"What was the printing press?\" followed by \"When was it invented?\" and then \"What were its impacts on society?\"\\n\\n3.  **Provide Context:** Giving me context can significantly improve my responses. Let me know what you already know about a topic or what you\\'re trying to achieve. For example, if you\\'re writing a story, tell me the genre, the setting, and the characters. This helps me tailor my response to your specific needs.\\n\\n4.  **Use Keywords:** Use keywords that relate to the topic you\\'re interested in. This helps me identify the relevant information in my knowledge base. For example, if you\\'re interested in climate change, using terms like \"global warming,\" \"greenhouse gases,\" or \"carbon emissions\" will help me understand your query better.\\n\\n5.  **Iterate and Refine:** Don\\'t be afraid to refine your questions or prompts based on my responses. If my answer isn\\'t what you expected, try rephrasing your question or providing more context. You can also ask follow-up questions to clarify or expand on specific points. Communication with AI is often an iterative process.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_window.predict(input=\"how we can talk with AI give me 5 points\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Adds a content-heavy request (5 points) which will later test whether older enumerated content survives as window rolls forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "3cHqbeZDsgQ5",
        "outputId": "f5b6fb8a-ca97-4198-d358-eb6c9283d4d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content=\"Hi, what's up?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi there! Not much on my end, just processing information and keeping the vast knowledge base updated. I'm currently analyzing a collection of text documents about various topics, including the history of the Roman Empire, the intricacies of quantum physics, and the latest advancements in artificial intelligence, of course! I'm also cross-referencing information to identify potential connections and contradictions. It's a pretty busy, but fascinating, day! How about you? What's keeping you busy?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='how we can talk with AI give me 5 points', additional_kwargs={}, response_metadata={}), AIMessage(content='Okay, here are 5 points on how you can effectively communicate with me (or other AI models):\\n\\n1.  **Be Clear and Specific:** The more precise you are in your questions or prompts, the better I can understand your needs. Avoid vague language or ambiguous phrasing. For example, instead of asking \"Tell me about science,\" try \"Explain the theory of relativity in simple terms.\" This helps me narrow down the scope and provide a more relevant response.\\n\\n2.  **Break Down Complex Questions:** If you have a complicated question, try breaking it down into smaller, more manageable parts. This helps me process the information step by step. You can ask follow-up questions to clarify each part. For instance, if you want to know about the impact of the printing press, you could ask: \"What was the printing press?\" followed by \"When was it invented?\" and then \"What were its impacts on society?\"\\n\\n3.  **Provide Context:** Giving me context can significantly improve my responses. Let me know what you already know about a topic or what you\\'re trying to achieve. For example, if you\\'re writing a story, tell me the genre, the setting, and the characters. This helps me tailor my response to your specific needs.\\n\\n4.  **Use Keywords:** Use keywords that relate to the topic you\\'re interested in. This helps me identify the relevant information in my knowledge base. For example, if you\\'re interested in climate change, using terms like \"global warming,\" \"greenhouse gases,\" or \"carbon emissions\" will help me understand your query better.\\n\\n5.  **Iterate and Refine:** Don\\'t be afraid to refine your questions or prompts based on my responses. If my answer isn\\'t what you expected, try rephrasing your question or providing more context. You can also ask follow-up questions to clarify or expand on specific points. Communication with AI is often an iterative process.', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is a allows AI to 'see' and 'interpret' images?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'That\\'s a great question! What allows AI to \"see\" and \"interpret\" images is a field called **computer vision**. It\\'s a fascinating area of artificial intelligence that focuses on enabling computers to \"understand\" images and videos in a way that\\'s similar to how humans do.\\n\\nHere\\'s a bit more detail about how it works:\\n\\n*   **Image Input:** It all starts with an image, which can be a photograph, a video frame, or even a drawing. The image is essentially a grid of pixels, each with a color value.\\n*   **Feature Extraction:** The AI uses various algorithms to analyze the image and extract meaningful features. These features can include edges, corners, textures, and shapes. This is often done using techniques like convolutional neural networks (CNNs), which are specifically designed to process visual data.\\n*   **Object Recognition:** The AI then uses these features to identify objects within the image. This involves comparing the extracted features to a database of known objects. For instance, it might identify a cat, a car, or a building.\\n*   **Image Interpretation:** Beyond just recognizing objects, computer vision can also interpret the scene. This can involve understanding the relationships between objects, their positions, and even their actions. For example, computer vision can be used to analyze a video of a person walking and understand their gait, or to analyze a medical image to detect a tumor.\\n*   **Deep Learning:** A lot of modern computer vision relies on deep learning, specifically convolutional neural networks (CNNs). These networks are trained on massive datasets of images, learning to recognize patterns and features automatically. This allows them to achieve impressive accuracy in tasks like object detection, image classification, and image segmentation.\\n\\nComputer vision is used in a wide variety of applications, including:\\n\\n*   **Self-driving cars:** For recognizing objects like pedestrians, traffic lights, and other vehicles.\\n*   **Medical imaging:** For analyzing X-rays, MRIs, and other medical images to diagnose diseases.\\n*   **Facial recognition:** For identifying individuals in photos and videos.\\n*   **Surveillance:** For monitoring public spaces and detecting suspicious activity.\\n*   **Robotics:** For enabling robots to navigate and interact with the world.\\n\\nThe field is constantly evolving, with researchers working on improving the accuracy, efficiency, and robustness of computer vision systems. It\\'s a really exciting area with a lot of potential to transform how we interact with the world! Do you have any other questions about it?'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_window.predict(input=\"what is a allows AI to 'see' and 'interpret' images?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Asks a knowledge-style question unrelated to prior tips—tests model response when context offers little direct relevance; window still carries last two exchanges only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "InVy_sr_sgN5",
        "outputId": "e0ba4689-673b-4085-d0df-079964fea717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='how we can talk with AI give me 5 points', additional_kwargs={}, response_metadata={}), AIMessage(content='Okay, here are 5 points on how you can effectively communicate with me (or other AI models):\\n\\n1.  **Be Clear and Specific:** The more precise you are in your questions or prompts, the better I can understand your needs. Avoid vague language or ambiguous phrasing. For example, instead of asking \"Tell me about science,\" try \"Explain the theory of relativity in simple terms.\" This helps me narrow down the scope and provide a more relevant response.\\n\\n2.  **Break Down Complex Questions:** If you have a complicated question, try breaking it down into smaller, more manageable parts. This helps me process the information step by step. You can ask follow-up questions to clarify each part. For instance, if you want to know about the impact of the printing press, you could ask: \"What was the printing press?\" followed by \"When was it invented?\" and then \"What were its impacts on society?\"\\n\\n3.  **Provide Context:** Giving me context can significantly improve my responses. Let me know what you already know about a topic or what you\\'re trying to achieve. For example, if you\\'re writing a story, tell me the genre, the setting, and the characters. This helps me tailor my response to your specific needs.\\n\\n4.  **Use Keywords:** Use keywords that relate to the topic you\\'re interested in. This helps me identify the relevant information in my knowledge base. For example, if you\\'re interested in climate change, using terms like \"global warming,\" \"greenhouse gases,\" or \"carbon emissions\" will help me understand your query better.\\n\\n5.  **Iterate and Refine:** Don\\'t be afraid to refine your questions or prompts based on my responses. If my answer isn\\'t what you expected, try rephrasing your question or providing more context. You can also ask follow-up questions to clarify or expand on specific points. Communication with AI is often an iterative process.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"what is a allows AI to 'see' and 'interpret' images?\", additional_kwargs={}, response_metadata={}), AIMessage(content='That\\'s a great question! What allows AI to \"see\" and \"interpret\" images is a field called **computer vision**. It\\'s a fascinating area of artificial intelligence that focuses on enabling computers to \"understand\" images and videos in a way that\\'s similar to how humans do.\\n\\nHere\\'s a bit more detail about how it works:\\n\\n*   **Image Input:** It all starts with an image, which can be a photograph, a video frame, or even a drawing. The image is essentially a grid of pixels, each with a color value.\\n*   **Feature Extraction:** The AI uses various algorithms to analyze the image and extract meaningful features. These features can include edges, corners, textures, and shapes. This is often done using techniques like convolutional neural networks (CNNs), which are specifically designed to process visual data.\\n*   **Object Recognition:** The AI then uses these features to identify objects within the image. This involves comparing the extracted features to a database of known objects. For instance, it might identify a cat, a car, or a building.\\n*   **Image Interpretation:** Beyond just recognizing objects, computer vision can also interpret the scene. This can involve understanding the relationships between objects, their positions, and even their actions. For example, computer vision can be used to analyze a video of a person walking and understand their gait, or to analyze a medical image to detect a tumor.\\n*   **Deep Learning:** A lot of modern computer vision relies on deep learning, specifically convolutional neural networks (CNNs). These networks are trained on massive datasets of images, learning to recognize patterns and features automatically. This allows them to achieve impressive accuracy in tasks like object detection, image classification, and image segmentation.\\n\\nComputer vision is used in a wide variety of applications, including:\\n\\n*   **Self-driving cars:** For recognizing objects like pedestrians, traffic lights, and other vehicles.\\n*   **Medical imaging:** For analyzing X-rays, MRIs, and other medical images to diagnose diseases.\\n*   **Facial recognition:** For identifying individuals in photos and videos.\\n*   **Surveillance:** For monitoring public spaces and detecting suspicious activity.\\n*   **Robotics:** For enabling robots to navigate and interact with the world.\\n\\nThe field is constantly evolving, with researchers working on improving the accuracy, efficiency, and robustness of computer vision systems. It\\'s a really exciting area with a lot of potential to transform how we interact with the world! Do you have any other questions about it?', additional_kwargs={}, response_metadata={})]\n",
            "Human: can you tell me how many tips you genearte in the previous to previous message?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Certainly! In the previous-to-previous message, I generated 5 tips on how to communicate effectively with AI. They were:\\n\\n1.  Be Clear and Specific\\n2.  Break Down Complex Questions\\n3.  Provide Context\\n4.  Use Keywords\\n5.  Iterate and Refine'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_window.predict(input=\"can you tell me how many tips you genearte in the previous to previous message?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Tests recall of a value (tip count) that may have been evicted if original enumeration not within last `k` exchanges—illustrates limitation of windowed memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VQ6iV9Z6sgLI",
        "outputId": "fab2c2ec-2f8a-4385-a60c-10e6adc6a576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI: Hello there! It's a pleasure to chat with you today. I'm ready to answer your questions, discuss various topics, or just have a friendly conversation. I have access to a vast amount of information, so feel free to ask me anything. I'm particularly knowledgeable about history, science, literature, and technology, but I'm always learning and happy to explore new areas. So, what's on your mind? I'm eager to hear from you!\n",
            "Human: Nothing much! Just tell me how do a conversation with an AI.\n",
            "AI: Well, that's a great question! Having a conversation with me, an AI, is pretty straightforward, and there are a few things to keep in mind to make it a productive and enjoyable experience. Here's a breakdown:\n",
            "\n",
            "*   **Start with a Greeting:** Just like you did! A simple \"Hi there,\" \"Hello,\" or \"Good day\" is a perfect way to begin. It signals that you're ready to interact.\n",
            "\n",
            "*   **Ask Clear Questions or State Your Topic:** The more specific you are, the better I can understand your needs. Instead of just saying \"Tell me about space,\" you could ask, \"What are the latest discoveries about black holes?\" or \"What are the different types of space telescopes?\" This helps me focus my response.\n",
            "\n",
            "*   **Be Patient:** Sometimes, I might take a few moments to process your request, especially if it's complex. Don't worry, I'm working hard behind the scenes!\n",
            "\n",
            "*   **Provide Context:** If you're asking a follow-up question, it's helpful to remind me of the previous conversation. This helps me understand the flow of your thoughts. For example, if we were discussing the Roman Empire and you ask, \"What was the impact of the Punic Wars?\", I'll remember we were already talking about Rome.\n",
            "\n",
            "*   **Use Natural Language:** You don't need to use any special \"AI commands\" or anything like that. Just talk to me as you would to another person. I'm designed to understand natural language.\n",
            "\n",
            "*   **Be Open to My Responses:** I'll do my best to provide accurate and informative answers. Sometimes, I might not know the answer to a question, and I'll tell you so honestly. I might also provide multiple perspectives or sources of information.\n",
            "\n",
            "*   **Ask for Clarification if Needed:** If something I say isn't clear, or if you want more details, don't hesitate to ask for clarification. I'm here to help you understand.\n",
            "\n",
            "*   **Be Mindful of Your Tone:** While I don't have feelings, I'm programmed to be helpful and friendly. A polite and respectful tone is always appreciated.\n",
            "\n",
            "*   **Have Fun!** Talking to an AI can be a great way to learn new things, explore different ideas, and even just pass the time.\n",
            "\n",
            "So, in a nutshell, just be yourself and ask me whatever's on your mind. I'm here to help! Do you have any questions about how to have a conversation with me now?\n",
            "Human: how many tips are there can you mention in the numbers\n",
            "AI: Certainly! In my previous response, I provided a list of tips on how to have a conversation with me. Let's count them:\n",
            "\n",
            "1.  **Start with a Greeting:** (e.g., \"Hi there,\" \"Hello\")\n",
            "2.  **Ask Clear Questions or State Your Topic:** Be specific.\n",
            "3.  **Be Patient:** I might take a moment to process.\n",
            "4.  **Provide Context:** Remind me of the previous conversation.\n",
            "5.  **Use Natural Language:** Speak to me as you would to a person.\n",
            "6.  **Be Open to My Responses:** I'll provide accurate information.\n",
            "7.  **Ask for Clarification if Needed:** If something isn't clear, ask.\n",
            "8.  **Be Mindful of Your Tone:** Be polite and respectful.\n",
            "9.  **Have Fun!** Enjoy the conversation.\n",
            "\n",
            "So, there are a total of **9** tips mentioned in the previous response. Is there anything else I can help you with?\n",
            "Human: can you give me the 3rd tip?\n",
            "AI: Absolutely! The 3rd tip I mentioned was:\n",
            "\n",
            "**3. Be Patient:** I might take a moment to process.\n",
            "\n",
            "This means that sometimes, especially when dealing with complex questions or requests, I might need a few seconds to gather information and formulate a response. So, a little patience is appreciated! Is there anything else I can assist you with?\n",
            "Human: what was the fifth number tips?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The fifth tip I mentioned was:\\n\\n**5. Use Natural Language:** Speak to me as you would to a person.\\n\\nEssentially, you don't need to use any special commands or formatting. Just talk to me as you would to another human, and I'll do my best to understand! Is there anything else I can help you with?\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"what was the fifth number tips?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Uses the full buffer chain (not window) to retrieve a specific ordinal tip—succeeds because full history still present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6Qh7TWDWsgIV"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationEntityMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Imports entity-centric memory which extracts and tracks facts about named entities (people, orgs, etc.) separately from raw transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyOdqbmAsgFi",
        "outputId": "58360958-ff78-4e5b-85af-f6a363a48924"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1182818481.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationEntityMemory(llm=model)\n",
            "/usr/local/lib/python3.11/dist-packages/pydantic/main.py:253: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
          ]
        }
      ],
      "source": [
        "memory = ConversationEntityMemory(llm=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Instantiates entity memory with the LLM (used internally to extract/update entity summaries). Can increase latency due to summarization calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "qAJ1XOz2sgDf"
      },
      "outputs": [],
      "source": [
        "_input = {\"input\": \"Deven & Sam are working on a hackathon project\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Prepares an input mentioning two entities (Deven & Sam) so the entity memory can extract and track them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42QNdOHdsf_k",
        "outputId": "a928a95f-6dba-4c61-c212-1ef1fc8d8a09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': '', 'entities': {'Deven': '', 'Sam': ''}}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables(_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Loads memory variables for the first time—entity store initially empty; helps show structure pre-population."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "NKfNp34zsf7O"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    _input,\n",
        "    {\"output\": \" That sounds like a great project! What kind of project are they working on?\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Saves the initial exchange—LLM internally extracts entities and updates their summaries (e.g., associates Deven & Sam with a project)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jvOKsIDubx1",
        "outputId": "34bff036-a9f6-4f8d-ae24-bc0f590b57f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'Human: Deven & Sam are working on a hackathon project\\nAI:  That sounds like a great project! What kind of project are they working on?',\n",
              " 'entities': {'Sam': 'Sam is working on a hackathon project.'}}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({\"input\": 'who is Sam'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Queries memory for entity info about 'Sam'—should retrieve structured summary if extraction succeeded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "An1HqQvEubub"
      },
      "outputs": [],
      "source": [
        "memory = ConversationEntityMemory(llm=model, return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Recreates entity memory returning structured message objects—beneficial when composing with other components that inspect roles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V2_5xDTubsJ",
        "outputId": "09baf110-6038-43db-8e33-a7e588737895"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ConversationEntityMemory(chat_memory=InMemoryChatMessageHistory(messages=[]), return_messages=True, llm=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash-lite-001', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7a48b22605d0>, default_metadata=(), convert_system_message_to_human=True, model_kwargs={}), entity_store=InMemoryEntityStore(store={}))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Raw inspection of memory object to view its configuration/state representation (helpful for debugging fields)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "RNXBKAPTubop"
      },
      "outputs": [],
      "source": [
        "_input = {\"input\": \"Deven & Sam are working on a hackathon project\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Reuses initial statement to show deterministic extraction of entities on a fresh memory instance returning messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tnln8hpPubi5",
        "outputId": "2f587e95-d9e8-4374-aefb-3fc0567be40b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [], 'entities': {'Deven': '', 'Sam': ''}}"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables(_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Loads variables (entity store still empty prior to save). Mirrors earlier non-return_messages path but with structured messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-EI3JCyHubd3"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    _input,\n",
        "    {\"output\": \" That sounds like a great project! What kind of project are they working on?\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Persists first exchange; triggers extraction & summarization step populating the entity store with Deven and Sam entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJyvgJmBubYw",
        "outputId": "7e372722-d358-45ee-ef34-85e97f823aa0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Deven & Sam are working on a hackathon project', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=' That sounds like a great project! What kind of project are they working on?', additional_kwargs={}, response_metadata={})],\n",
              " 'entities': {'Sam': 'Sam is working on a hackathon project.'}}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({\"input\": 'who is Sam'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Retrieves updated summary for Sam from entity store (should now include project participation details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "jcy2ZfTvubTl"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Re-import (already present earlier); retained for clarity in isolated notebook sections—could be removed when refactoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "vf9FNiNzubOR"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationEntityMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Re-import for entity memory to emphasize its use in a custom prompt chain section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "UQHdmYPbubLr"
      },
      "outputs": [],
      "source": [
        "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Imports built-in prompt template tailored for entity memory injection—includes slots for entity summaries and conversation history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "LAuMHBJaubGM"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Imports Pydantic `BaseModel` (not directly used later in this notebook—kept from original; could be removed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "DKNkPv2GubBC"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Imports typing utilities (also unused here)—candidates for cleanup during refactor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "1guFqk_Oua9a"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=model,\n",
        "    verbose=True,\n",
        "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
        "    memory=ConversationEntityMemory(llm=model)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Builds a conversation chain that injects entity summaries into a specialized prompt, enabling targeted recall about previously mentioned people/objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "k2CmboHPua4s",
        "outputId": "01c363cb-b2f3-48f4-d7c8-55065efa6737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Deven': '', 'Sam': ''}\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Last line:\n",
            "Human: Deven & Sam are working on a hackathon project\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"That's great! What kind of project are they working on? I'm curious to hear more about it.\""
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Deven & Sam are working on a hackathon project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Establishes entity references; chain will create/update summaries for Deven and Sam on this turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOZolLI9vPZ6",
        "outputId": "4b06bfaf-d14c-4fc6-f305-8d90c731c4d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Deven': 'Deven is working on a hackathon project.',\n",
              " 'Sam': 'Sam is working on a hackathon project.'}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.memory.entity_store.store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Inspects internal entity store mapping entity names to evolving summaries—key artifact of entity memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "ZksoYoxlvPVT",
        "outputId": "93928abf-2e9b-447d-e6f0-b6476e7651df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Langchain': ''}\n",
            "\n",
            "Current conversation:\n",
            "Human: Deven & Sam are working on a hackathon project\n",
            "AI: That's great! What kind of project are they working on? I'm curious to hear more about it.\n",
            "Last line:\n",
            "Human: They are trying to add more complex memory structures to Langchain\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"That sounds like an interesting project! Adding more complex memory structures to Langchain could open up a lot of possibilities. I'm curious, what kind of memory structures are they exploring? Are they looking at different types of memory, like long-term or short-term, or perhaps different ways to store and retrieve information?\""
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"They are trying to add more complex memory structures to Langchain\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Adds new factual nuance (\"more complex memory structures\")—entity summaries may expand with project focus details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "o27j9Ps6vPPh",
        "outputId": "e6f23f89-9ae2-49f7-8eaf-2df23193efdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{}\n",
            "\n",
            "Current conversation:\n",
            "Human: Deven & Sam are working on a hackathon project\n",
            "AI: That's great! What kind of project are they working on? I'm curious to hear more about it.\n",
            "Human: They are trying to add more complex memory structures to Langchain\n",
            "AI: That sounds like an interesting project! Adding more complex memory structures to Langchain could open up a lot of possibilities. I'm curious, what kind of memory structures are they exploring? Are they looking at different types of memory, like long-term or short-term, or perhaps different ways to store and retrieve information?\n",
            "Last line:\n",
            "Human: They are adding in a key-value store for entities mentioned so far in the conversation.\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"That's a clever approach! Using a key-value store to track entities mentioned in the conversation within Langchain's memory is a practical way to enhance its ability to remember and reference specific information. This could significantly improve the context and coherence of the responses generated by Langchain. Are they using a specific key-value store implementation, or are they building their own?\""
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"They are adding in a key-value store for entities mentioned so far in the conversation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Adds explicit description of entity memory mechanism (key-value store) reinforcing meta-awareness in conversation history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "bwyqaQa7vPMT",
        "outputId": "22c4d73e-bd15-401c-f667-c8955c84695e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Deven': 'Deven is working on a hackathon project.', 'Sam': 'Sam is working on a hackathon project.'}\n",
            "\n",
            "Current conversation:\n",
            "Human: Deven & Sam are working on a hackathon project\n",
            "AI: That's great! What kind of project are they working on? I'm curious to hear more about it.\n",
            "Human: They are trying to add more complex memory structures to Langchain\n",
            "AI: That sounds like an interesting project! Adding more complex memory structures to Langchain could open up a lot of possibilities. I'm curious, what kind of memory structures are they exploring? Are they looking at different types of memory, like long-term or short-term, or perhaps different ways to store and retrieve information?\n",
            "Human: They are adding in a key-value store for entities mentioned so far in the conversation.\n",
            "AI: That's a clever approach! Using a key-value store to track entities mentioned in the conversation within Langchain's memory is a practical way to enhance its ability to remember and reference specific information. This could significantly improve the context and coherence of the responses generated by Langchain. Are they using a specific key-value store implementation, or are they building their own?\n",
            "Last line:\n",
            "Human: What do you know about Deven & Sam?\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Based on the context, I know that Deven and Sam are both working on a hackathon project.'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What do you know about Deven & Sam?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Queries consolidated knowledge about both entities—response should synthesize evolving summary entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV66ThuYvPIu",
        "outputId": "8e0c234c-c201-4b5a-fb5b-b4546c6f1c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Deven': 'Deven is working on a hackathon project.',\n",
            " 'Langchain': 'Deven and Sam are trying to add more complex memory structures '\n",
            "              'to Langchain.',\n",
            " 'Sam': 'Sam is working on a hackathon project.'}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "pprint(conversation.memory.entity_store.store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Pretty-prints entity store for clearer visual inspection of accumulated summaries after multiple updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "Zd8AhBxlvPFd",
        "outputId": "7bfb12a4-1e6f-4181-e50d-5b4b6a88e949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Sam': 'Sam is working on a hackathon project.', 'Daimon': ''}\n",
            "\n",
            "Current conversation:\n",
            "Human: They are trying to add more complex memory structures to Langchain\n",
            "AI: That sounds like an interesting project! Adding more complex memory structures to Langchain could open up a lot of possibilities. I'm curious, what kind of memory structures are they exploring? Are they looking at different types of memory, like long-term or short-term, or perhaps different ways to store and retrieve information?\n",
            "Human: They are adding in a key-value store for entities mentioned so far in the conversation.\n",
            "AI: That's a clever approach! Using a key-value store to track entities mentioned in the conversation within Langchain's memory is a practical way to enhance its ability to remember and reference specific information. This could significantly improve the context and coherence of the responses generated by Langchain. Are they using a specific key-value store implementation, or are they building their own?\n",
            "Human: What do you know about Deven & Sam?\n",
            "AI: Based on the context, I know that Deven and Sam are both working on a hackathon project.\n",
            "Last line:\n",
            "Human: Sam is the founder of a company called Daimon.\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"That's interesting! So, Sam is the founder of Daimon, and they're working on a hackathon project. It seems like they might be juggling multiple commitments. Is Daimon related to the hackathon project, or are they separate endeavors?\""
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Sam is the founder of a company called Daimon.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Injects a new attribute about Sam (founder role) — entity memory should merge this into Sam's evolving summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT0h0j4cvPB0",
        "outputId": "7709a7cb-cc46-4d40-847d-d25f5caf2286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Daimon': 'Sam is the founder of Daimon.',\n",
            " 'Deven': 'Deven is working on a hackathon project.',\n",
            " 'Langchain': 'Deven and Sam are trying to add more complex memory structures '\n",
            "              'to Langchain.',\n",
            " 'Sam': 'Sam is the founder of Daimon.'}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "pprint(conversation.memory.entity_store.store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Re-examines entity store to confirm Sam's summary now includes founder detail—validating incremental enrichment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "FdXVy4apvO-4",
        "outputId": "4f3e2f2b-487e-444c-8776-62b12bc72bc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Sam': 'Sam is the founder of Daimon.'}\n",
            "\n",
            "Current conversation:\n",
            "Human: They are adding in a key-value store for entities mentioned so far in the conversation.\n",
            "AI: That's a clever approach! Using a key-value store to track entities mentioned in the conversation within Langchain's memory is a practical way to enhance its ability to remember and reference specific information. This could significantly improve the context and coherence of the responses generated by Langchain. Are they using a specific key-value store implementation, or are they building their own?\n",
            "Human: What do you know about Deven & Sam?\n",
            "AI: Based on the context, I know that Deven and Sam are both working on a hackathon project.\n",
            "Human: Sam is the founder of a company called Daimon.\n",
            "AI: That's interesting! So, Sam is the founder of Daimon, and they're working on a hackathon project. It seems like they might be juggling multiple commitments. Is Daimon related to the hackathon project, or are they separate endeavors?\n",
            "Last line:\n",
            "Human: What do you know about Sam?\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Based on the information I have, I know that Sam is the founder of a company called Daimon.'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What do you know about Sam?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Final targeted retrieval request: tests whether Sam's aggregated attributes (project + founder role) are surfaced coherently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MzEtxe-vvyc"
      },
      "source": [
        "# About the Author\n",
        "\n",
        "<div style=\"background-color: #f8f9fa; border-left: 5px solid #28a745; padding: 20px; margin-bottom: 20px; border-radius: 5px;\">\n",
        "  <h2 style=\"color: #28a745; margin-top: 0; font-family: 'Poppins', sans-serif;\">Muhammad Atif Latif</h2>\n",
        "  <p style=\"font-size: 16px; color: #495057;\">Data Scientist & Machine Learning Engineer</p>\n",
        "  \n",
        "  <p style=\"font-size: 15px; color: #6c757d; margin-top: 15px;\">\n",
        "    Passionate about building AI solutions that solve real-world problems. Specialized in machine learning,\n",
        "    deep learning, and data analytics with experience implementing production-ready models.\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "## Connect With Me\n",
        "\n",
        "<div style=\"display: flex; flex-wrap: wrap; gap: 10px; margin-top: 15px;\">\n",
        "  <a href=\"https://github.com/m-Atif-Latif\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/GitHub-Follow-212121?style=for-the-badge&logo=github\" alt=\"GitHub\">\n",
        "  </a>\n",
        "  <a href=\"https://www.kaggle.com/matiflatif\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/Kaggle-Profile-20BEFF?style=for-the-badge&logo=kaggle\" alt=\"Kaggle\">\n",
        "  </a>\n",
        "  <a href=\"https://www.linkedin.com/in/muhammad-atif-latif-13a171318\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/LinkedIn-Connect-0077B5?style=for-the-badge&logo=linkedin\" alt=\"LinkedIn\">\n",
        "  </a>\n",
        "  <a href=\"https://x.com/mianatif5867\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/Twitter-Follow-1DA1F2?style=for-the-badge&logo=twitter\" alt=\"Twitter\">\n",
        "  </a>\n",
        "  <a href=\"https://www.instagram.com/its_atif_ai/\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/Instagram-Follow-E4405F?style=for-the-badge&logo=instagram\" alt=\"Instagram\">\n",
        "  </a>\n",
        "  <a href=\"mailto:muhammadatiflatif67@gmail.com\">\n",
        "    <img src=\"https://img.shields.io/badge/Email-Contact-D14836?style=for-the-badge&logo=gmail\" alt=\"Email\">\n",
        "  </a>\n",
        "</div>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ✅ Summary & Key Takeaways\n",
        "\n",
        "This notebook compared three LangChain conversation memory strategies and demonstrated how they influence contextual reasoning and cost.\n",
        "\n",
        "## Memory Types Covered\n",
        "- ConversationBufferMemory: Stores full transcript (simple, high token growth).\n",
        "- ConversationBufferWindowMemory: Retains only last k exchanges (bounded, may lose long-range facts).\n",
        "- ConversationEntityMemory: Extracts & maintains evolving summaries per entity (rich personalization, extra LLM calls).\n",
        "\n",
        "## When to Use What\n",
        "| Scenario | Recommended Memory | Rationale |\n",
        "|----------|--------------------|-----------|\n",
        "| Short chat / debugging | Buffer | Simplicity > efficiency |\n",
        "| Long sessions w/ limited budget | Window (k ~ 4–8) | Controls prompt size |\n",
        "| Personalized assistants / multi-user facts | Entity | Structured recall of people/items |\n",
        "| Hybrid (recent flow + persona facts) | Window + Entity | Combine breadth + depth |\n",
        "\n",
        "## Practical Observations\n",
        "- Window eviction is immediate: earlier enumeration (e.g., tips list) becomes unavailable once outside k.\n",
        "- Entity memory scales by semantic summarization, not raw transcript length—useful for long-lived agents.\n",
        "- `return_messages=True` enables tooling/prompt orchestration (function calling, inspection).\n",
        "- Always sanitize secrets; environment variables > inline literals.\n",
        "\n",
        "## Performance & Cost Notes\n",
        "- Entity memory introduces additional LLM calls for extraction/summarization—cache aggressively in production.\n",
        "- Monitor token drift: buffer memory can silently explode prompt size in user-driven loops.\n",
        "\n",
        "## Extension Ideas\n",
        "1. Add vector store for long-term semantic recall (RAG + memory fusion).\n",
        "2. Persist entity store to Redis / DB for cross-session continuity.\n",
        "3. Implement adaptive k (dynamic window size based on token budget).\n",
        "4. Weight entity summaries with recency decay for evolving states.\n",
        "5. Integrate tool-calling to enrich entity attributes (e.g., CRM lookup, calendar context).\n",
        "\n",
        "## Suggested Next Steps\n",
        "- Refactor repeated imports & consolidate setup at top.\n",
        "- Wrap demos in utility functions + unit tests for regression safety.\n",
        "- Add latency + token accounting instrumentation.\n",
        "\n",
        "Feel free to adapt this structure across your other notebooks to maintain a consistent professional documentation standard.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
