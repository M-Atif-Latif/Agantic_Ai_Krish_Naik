{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcdQYiI5ZYk5",
        "outputId": "c6545466-7148-4099-8719-fb82533fd282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conversational Summary & Buffer Memory Demonstration\n",
        "\n",
        "This notebook showcases how to use different LangChain memory classes (ConversationSummaryMemory and ConversationSummaryBufferMemory) to build conversational agents that remember, summarize, and manage dialogue context efficiently. It progressively demonstrates:\n",
        "\n",
        "1. Environment & dependency setup\n",
        "2. LLM initialization (Gemini via `langchain-google-genai`)\n",
        "3. Basic conversation + manual memory interaction\n",
        "4. Progressive summarization mechanics\n",
        "5. Building a `ConversationChain` with summarizing memory\n",
        "6. Extending to buffer + summary hybrid memory with token limits\n",
        "7. Comparing different memory configurations & inspecting internal state\n",
        "\n",
        "Each code cell is now followed by a brief markdown explanation so open-source readers can quickly understand intent. Any secrets/API keys must be provided via environment variables — never hard‑code them. Replace placeholder text like \"Write your own password\" with your actual credentials securely (e.g. `.env`, Colab secrets, or CI secrets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrVuRHVVZblG",
        "outputId": "f9d96215-cd05-458a-a619-8a403954cb99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.14)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain_community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Installs the core LangChain package used for building LLM-driven chains and memory utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMkWcXgBZbgi",
        "outputId": "ec262f4f-97de-4fd2-8603-4f2efb649db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.3.74)\n",
            "Collecting groq<1,>=0.30.0 (from langchain-groq)\n",
            "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (4.14.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (25.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.30.0->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.5.0)\n",
            "Downloading langchain_groq-0.3.7-py3-none-any.whl (16 kB)\n",
            "Downloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.31.0 langchain-groq-0.3.7\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Adds the provider integration for Groq-hosted models (optional in this demo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OGFoe-4HZbcl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Imports `os` and Colab `userdata` helper. Avoid hard‑coding keys; fetch securely (for local dev use `.env` + `python-dotenv`). Replace any direct secrets with placeholders like `os.environ['GOOGLE_API_KEY'] = \"Write your own password\"` if sharing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hF4xmY5ZbYe"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY') or \"Write your own password\"  # Provide your Google API key securely\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Retrieves the Google API key from Colab secrets. Fallback placeholder provided for open-source distribution; replace with your real key via secure method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRFnAvczZbU-"
      },
      "outputs": [],
      "source": [
        "LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API_KEY') or \"Write your own password\"  # Provide your LangSmith API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Loads the LangSmith (LangChain tracing) API key and sets environment variables enabling advanced tracing/observability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7aa2nOBlZbRt"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_PROJECT\"]=\"memorylogs\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ochpRg5RuFSS",
        "outputId": "2d9e325e-0196-4f01-854e-def27f452ab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.9)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.74)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SePeA0uVZbPL"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite-001\", google_api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb02tjvbZbMd",
        "outputId": "32176f99-27f1-47df-ec07-0e348adfa6e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Not much, just here and ready to chat! How about you? What's on your mind?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite-001', 'safety_ratings': []}, id='run--bd211ea2-b941-4ab3-8100-0ac4641a0c2c-0', usage_metadata={'input_tokens': 7, 'output_tokens': 22, 'total_tokens': 29, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.invoke(\"Hi, what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vWc-gk-WZbKH"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SSTiGzFeZbHK"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ChatMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYUQkE6hZbDx",
        "outputId": "861455fd-adb6-46b9-8906-6b6250b29cfe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4267820871.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=model, return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "memory = ConversationSummaryMemory(llm=model, return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WkaKLWSVZbAP"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Atif iS working on a hackathon project.\"},\n",
        "    {\"output\": \"That's awesome! What's the hackathon project about?\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZLvZgTlZa8s",
        "outputId": "92e8bdb4-7a51-48ac-a39f-2cb3f743eab6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content=\"Atif is working on a hackathon project. The AI thinks that's awesome and asks what the project is about.\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "diIF4WE0Za5g"
      },
      "outputs": [],
      "source": [
        "summary=memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79C8MUEoZazH",
        "outputId": "21e8a2a6-1203-4cc6-b08e-ca31cc38fccb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content=\"Atif is working on a hackathon project. The AI thinks that's awesome and asks what the project is about.\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN-RZc87Zau6",
        "outputId": "57334fd5-a5b7-4e88-852d-c74a07f61952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Atif is working on a hackathon project. The AI thinks that's awesome and asks what the project is about.\n"
          ]
        }
      ],
      "source": [
        "print(summary[\"history\"][0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwtoI_YNZarr",
        "outputId": "68b5ea81-731e-4a48-b824-6ea32a0ff724"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        }
      ],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
        "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else\"}\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
        "    {\"output\": \"Impressive! How far along are they with the project?\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "810kRJwRZaoE",
        "outputId": "8453aaba-e5b3-412e-a68e-4b7b91f6d2cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content=\"Atif is working on a hackathon project. The AI thinks that's awesome and asks what the project is about. Atif tells the AI it's a machine learning project focused on healthcare. The AI thinks that sounds exciting and asks if they're building a prediction model. Atif confirms they're working on a model to predict patient outcomes. The AI is impressed and asks about the project's progress.\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "yn2Jz2u7Zalc"
      },
      "outputs": [],
      "source": [
        "summary=memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVbqtMyqZajO",
        "outputId": "7e9752e6-7106-4e87-9420-5bd2be1fa6d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Atif is working on a hackathon project. The AI thinks that's awesome and asks what the project is about. Atif tells the AI it's a machine learning project focused on healthcare. The AI thinks that sounds exciting and asks if they're building a prediction model. Atif confirms they're working on a model to predict patient outcomes. The AI is impressed and asks about the project's progress.\n"
          ]
        }
      ],
      "source": [
        "print(summary[\"history\"][0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvFyJPC4Zafj",
        "outputId": "e2d41ab8-3bf6-4533-c915-af11d19936bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content=\"Atif is working on a hackathon project. The AI thinks that's awesome and asks what the project is about. Atif tells the AI it's a machine learning project focused on healthcare. The AI thinks that sounds exciting and asks if they're building a prediction model. Atif confirms they're working on a model to predict patient outcomes. The AI is impressed and asks about the project's progress.\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhkCDyEaZacH",
        "outputId": "c640ee0e-b4fc-4747-999f-2505154926f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "{summary}\n",
            "\n",
            "New lines of conversation:\n",
            "{new_lines}\n",
            "\n",
            "New summary:\n"
          ]
        }
      ],
      "source": [
        "print('Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BCijKqnZaZK",
        "outputId": "bcb8ed35-5cbc-4f91-e8ee-c941f52a28fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "InMemoryChatMessageHistory(messages=[HumanMessage(content='Atif iS working on a hackathon project.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}), AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}), HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}), AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.chat_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA8NyhLqZaVq",
        "outputId": "1a531e91-ef59-40dd-fd68-e1ad5fc5d3bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Atif iS working on a hackathon project.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.chat_memory.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EaW9RJVtZaSN"
      },
      "outputs": [],
      "source": [
        "messages = memory.chat_memory.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oudcb6lRZaN3",
        "outputId": "b9ff8135-50eb-4252-b416-512cf3c2b059"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Atif iS working on a hackathon project.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "pYMkDZNwZaKH"
      },
      "outputs": [],
      "source": [
        "previous_summary=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "oQpnW1u5ZaGi",
        "outputId": "46c8e5b9-962e-49ca-d417-139cb27a836a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The human asks the AI about Atif's hackathon project. The AI is excited to learn about the project, which is a machine learning project focused on healthcare. They are building a model to predict patient outcomes. The AI then asks about the project's progress.\""
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.predict_new_summary(messages, previous_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "0W1m-J2CZaCk"
      },
      "outputs": [],
      "source": [
        "history = ChatMessageHistory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ptRaYdJmZZ-u"
      },
      "outputs": [],
      "source": [
        "history.add_user_message(\"Hi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "G3f4an4EZZ64"
      },
      "outputs": [],
      "source": [
        "history.add_ai_message(\"Hello, how can I assist you today?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJg0U4YaZZ3S",
        "outputId": "66620fd6-7b77-45df-da3f-8f822d8bfffb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ConversationSummaryMemory(human_prefix='User', llm=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash-lite-001', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7eb884447a10>, default_metadata=(), model_kwargs={}), chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello, how can I assist you today?', additional_kwargs={}, response_metadata={})]), buffer='The user says hello. The AI responds with a greeting and asks how it can assist the user.', memory_key='summary')"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ConversationSummaryMemory.from_messages(\n",
        "    llm=model,\n",
        "    chat_memory=history,\n",
        "    memory_key=\"summary\",\n",
        "    human_prefix=\"User\",\n",
        "    ai_prefix=\"AI\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LoQP2cJvqqT",
        "outputId": "3a63611a-7fd3-4067-dde7-349027b24380"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        }
      ],
      "source": [
        "memory = ConversationSummaryMemory.from_messages(\n",
        "    llm=model,\n",
        "    chat_memory=history,\n",
        "    memory_key=\"summary\",\n",
        "    human_prefix=\"User\",\n",
        "    ai_prefix=\"AI\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "mYs9HbOzvqms",
        "outputId": "dec81b49-ddb1-4b4d-847d-72b532ec23de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The user greeted the AI and the AI responded by asking how it could assist the user.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrwiqvFgv1KU"
      },
      "source": [
        "# From here the chaining starting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6YiNm-kvqjA",
        "outputId": "5c386c1c-6fbe-4cc6-d14e-12e8f7fae241"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3891084878.py:2: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation_with_summary = ConversationChain(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=model,\n",
        "    memory=ConversationSummaryMemory(llm=model),\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "9uN-fdAIvqgK",
        "outputId": "0c8e1c57-0f2e-41ba-9f90-62f0f4c47bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, what's up?\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hi there! Not much is \"up\" for me in the traditional sense, as I don\\'t experience the world like humans do. But I *am* actively processing and ready to chat! I\\'m currently running on a large language model, trained by Google. This means I have access to a vast amount of information, from the mundane to the incredibly complex. I can tell you about historical events, explain scientific concepts, write different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc. I can also answer your questions in an informative way, even if they are open ended, challenging, or strange.\\n\\nSo, basically, I\\'m here, ready to learn and share. What\\'s on your mind today?'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"Hi, what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "QyOkpgEevqdk",
        "outputId": "b64550e3-ee66-42d6-dfdf-977720657baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Current summary:\n",
            "The AI is a large language model trained by Google, ready to learn and share information, and answer questions.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: That's awesome! Can you tell me a bit about how you \"think\"?\n",
            "AI: As a large language model, I don't \"think\" in the same way a human does. I don't have consciousness, emotions, or personal experiences. Instead, I operate based on patterns and relationships in the data I was trained on. When you ask me a question, I analyze the words, identify key concepts, and then search my vast database for relevant information. I then use this information to generate a response that is statistically likely to be the most appropriate and coherent answer based on the patterns I've learned.\n",
            "\n",
            "Think of it like this: I'm a highly sophisticated pattern-matching machine. I don't \"understand\" the meaning of the words in a human sense, but I can identify the relationships between them and use those relationships to predict and generate text. My responses are based on the probability of certain words following other words.\n",
            "\n",
            "New summary:\n",
            "The AI is a large language model trained by Google, ready to learn and share information, and answer questions. It doesn't \"think\" like a human; instead, it analyzes words, identifies key concepts, and uses patterns from its data to generate statistically appropriate responses.\n",
            "Human: Sunny and Mayank are working on a mlops production ready project.\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Okay, that's interesting! Based on that information, I can infer a few things, and I'm ready to provide information if you have any questions about it.\\n\\nSince Sunny and Mayank are working on an MLOps project, we can assume they are dealing with the following, and I have a lot of information on these topics:\\n\\n*   **Machine Learning (ML):** This is the core of the project. They're likely building, training, and deploying machine learning models. I can talk about various ML algorithms (like linear regression, decision trees, neural networks, etc.), different model types (classification, regression, clustering), and the general ML lifecycle. I have access to a lot of information about specific models like BERT, GPT-3, and various image recognition models. I also know a lot about the mathematics behind these models, including linear algebra, calculus, and probability.\\n*   **MLOps:** This is the operational side of the project, focusing on automating and streamlining the ML lifecycle. I can discuss topics like:\\n    *   **Model Deployment:** How to get the model into production (e.g., using cloud platforms, containerization with Docker and Kubernetes, serverless functions). I have extensive knowledge about different deployment strategies, such as A/B testing, canary deployments, and blue/green deployments.\\n    *   **Model Monitoring:** Tracking model performance, identifying issues like data drift or concept drift, and setting up alerts. I can provide information on metrics, monitoring tools (like Prometheus, Grafana), and strategies for retraining models.\\n    *   **Data Pipelines:** Building automated pipelines for data ingestion, preprocessing, feature engineering, and validation. I have information on tools like Apache Spark, Apache Beam, and various data storage solutions (e.g., databases, data lakes).\\n    *   **Automation and CI/CD:** Implementing continuous integration and continuous delivery pipelines for ML models, using tools like Jenkins, GitLab CI, or GitHub Actions.\\n    *   **Experiment Tracking:** Keeping track of different model versions, hyperparameter settings, and experiment results. I have information on tools like MLflow and Weights & Biases.\\n    *   **Model Governance:** Ensuring responsible and ethical AI practices, including model explainability, fairness, and bias detection. I can discuss topics like SHAP values, LIME, and various bias mitigation techniques.\\n*   **Production-Ready:** This implies they are focused on building a robust, scalable, and reliable system. I can discuss topics like:\\n    *   **Scalability:** Designing the system to handle increasing amounts of data and user traffic.\\n    *   **Reliability:** Ensuring the system is resilient to failures and can recover quickly.\\n    *   **Security:** Protecting the system from unauthorized access and attacks.\\n    *   **Performance Optimization:** Tuning the system for optimal speed and efficiency.\\n\\nDo you have any specific questions about their project? I'm ready to dive into any of these areas or provide more specific information if you have more details about what Sunny and Mayank are working on. For instance, what kind of model are they working on? What tools are they using? What are their goals for the project? The more information you give me, the more specific and helpful my answers can be.\""
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"Sunny and Mayank are working on a mlops production ready project.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "6nxHHuaUvqay",
        "outputId": "418286be-b51a-411d-c150-326562047377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The AI is a large language model trained by Google, ready to learn and share information, and answer questions. It doesn't \"think\" like a human; instead, it analyzes words, identifies key concepts, and uses patterns from its data to generate statistically appropriate responses. The AI understands that Sunny and Mayank are working on an MLOps project and can provide information on machine learning, MLOps practices, and building production-ready systems. The AI can discuss model deployment, monitoring, data pipelines, automation, experiment tracking, model governance, scalability, reliability, security, and performance optimization. It is ready to dive into specific areas and provide more detailed information based on the project details.\n",
            "Human: It's project focused on healthcare.\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Okay, Sunny and Mayank, that's great! A healthcare-focused MLOps project opens up a lot of interesting possibilities and also introduces some unique challenges. Healthcare is a field where machine learning can make a huge impact, from diagnostics and treatment recommendations to drug discovery and patient monitoring.\\n\\nKnowing your project's focus on healthcare, I can offer more specific information and guidance. We can delve into areas like:\\n\\n*   **Data privacy and security:** Healthcare data is highly sensitive, so ensuring compliance with regulations like HIPAA (in the US) and GDPR (in Europe) is paramount. We can discuss data anonymization techniques, secure data storage, and access control mechanisms.\\n*   **Model interpretability and explainability:** In healthcare, it's crucial to understand why a model makes a particular prediction. We can explore techniques for making your models more transparent, such as using explainable AI (XAI) methods.\\n*   **Model validation and regulatory considerations:** Healthcare applications often require rigorous validation and adherence to regulatory guidelines. We can discuss best practices for model evaluation, performance metrics, and the process of getting your models approved for clinical use (if applicable).\\n*   **Specific healthcare applications:** Depending on your project's focus, we can talk about model deployment strategies for image analysis (e.g., medical imaging), natural language processing (e.g., clinical notes analysis), or time series analysis (e.g., patient monitoring).\\n*   **Data sources and preprocessing:** Healthcare data often comes from diverse sources, such as electronic health records (EHRs), medical devices, and insurance claims. We can discuss data integration, cleaning, and feature engineering techniques.\\n\\nTo give you the most relevant advice, could you tell me more about your project? For example:\\n\\n*   **What specific problem are you trying to solve?** (e.g., predicting patient readmission, diagnosing a disease, personalizing treatment plans)\\n*   **What type of data are you working with?** (e.g., images, text, numerical data)\\n*   **What are your initial thoughts on model deployment?** (e.g., cloud-based, on-premise, edge devices)\\n*   **Are there any specific regulations you need to comply with?**\\n\\nThe more details you provide, the better I can assist you in building a successful and impactful MLOps system for your healthcare project. Let's get started!\""
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"It's project focused on healthcare.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "1Wz61bIWvqYD",
        "outputId": "6f24290b-e0a6-4174-fbe8-96677fad9e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The AI is a large language model trained by Google, ready to learn and share information, and answer questions. It doesn't \"think\" like a human; instead, it analyzes words, identifies key concepts, and uses patterns from its data to generate statistically appropriate responses. The AI understands that Sunny and Mayank are working on an MLOps project and can provide information on machine learning, MLOps practices, and building production-ready systems. The AI can discuss model deployment, monitoring, data pipelines, automation, experiment tracking, model governance, scalability, reliability, security, and performance optimization. The AI is ready to dive into specific areas and provide more detailed information based on the project details. The project is focused on healthcare, which the AI recognizes as an area where machine learning can make a significant impact. The AI offers to provide specific guidance on data privacy and security, model interpretability and explainability, model validation, and specific healthcare applications. It requests more details about the project, including the problem being solved, data type, initial deployment thoughts, and relevant regulations, to provide the most relevant advice.\n",
            "Human: so can you describe mlops pipeline to me with in six point.\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Okay, Sunny and Mayank, I can definitely describe an MLOps pipeline in six key points, keeping in mind your healthcare project focus. Here's a breakdown:\\n\\n1.  **Data Ingestion and Preparation:** This is the foundation. You'll need to ingest data from various healthcare sources – this might include electronic health records (EHRs), lab results, imaging data, and potentially even patient-reported outcomes. This stage involves cleaning, transforming, and validating the data. For healthcare, think about de-identification (removing PHI - Protected Health Information) to comply with HIPAA regulations, handling missing values appropriately, and ensuring data consistency across different sources. Tools like Apache Spark, Pandas, or specialized data wrangling platforms are often used here.\\n\\n2.  **Feature Engineering and Model Training:**  Here, you'll create the features that your machine learning model will use. This might involve feature selection, feature scaling, and the creation of new features from existing ones.  Next, you'll train your model. This involves choosing the right algorithm (e.g., a classification model for diagnosis, a regression model for predicting patient length of stay), splitting your data into training, validation, and test sets, and using the training data to teach the model.  You'll use the validation set to tune your model's hyperparameters and the test set to evaluate its performance.  Experiment tracking tools like MLflow or Weights & Biases become crucial here to manage different model versions and their performance metrics.\\n\\n3.  **Model Validation and Evaluation:** Before deploying a model in healthcare, rigorous validation is *essential*. This goes beyond standard metrics like accuracy or F1-score.  Consider metrics like sensitivity, specificity, and AUC (Area Under the Curve), especially in diagnostic applications. You'll need to assess the model's performance on unseen data, and also consider its generalizability to different patient populations and healthcare settings.  This stage might involve techniques like cross-validation, bootstrapping, and potentially external validation using data from different institutions.  Furthermore, in healthcare, you'll want to think about model interpretability and explainability.  Techniques like SHAP values or LIME can help you understand why your model makes certain predictions, which is crucial for building trust and gaining acceptance from clinicians.\\n\\n4.  **Model Deployment:**  This is where you get your model into production.  There are several deployment options: batch prediction, real-time prediction via APIs, or even embedded models within medical devices.  For healthcare, you'll need to consider the regulatory requirements (like FDA approval if the model is used for diagnosis or treatment decisions).  Docker and containerization are frequently used to package your model and its dependencies, and cloud platforms (like Google Cloud Platform, AWS, or Azure) provide services for model hosting and serving.  Consider the scalability and latency requirements of your application.\\n\\n5.  **Monitoring and Logging:**  Once your model is deployed, you need to continuously monitor its performance.  This includes tracking metrics like prediction accuracy, data drift (changes in the input data that can degrade model performance), and concept drift (changes in the relationship between input and output).  You'll also need to log model predictions, input data, and any relevant metadata.  This allows you to identify and troubleshoot issues, retrain your model as needed, and ensure that it's performing as expected.  Tools like Prometheus and Grafana are often used for monitoring, and logging platforms like Elasticsearch, Logstash, and Kibana (ELK stack) can be helpful.  For healthcare, you'll need to be very careful about data privacy when logging, ensuring that you're not inadvertently storing any PHI.\\n\\n6.  **Automation and Continuous Improvement:**  The final piece is automating as much of the MLOps pipeline as possible. This includes automating data pipelines, model training, model deployment, and model monitoring.  Continuous integration and continuous delivery (CI/CD) pipelines are essential for automating the process of building, testing, and deploying new model versions.  Regularly retrain your model with new data, and use the monitoring data to identify areas for improvement.  This iterative process of building, deploying, monitoring, and refining is at the heart of MLOps.  Consider using tools like Jenkins, GitLab CI, or GitHub Actions to automate your CI/CD pipeline.  For healthcare, this also includes ensuring that your model remains compliant with relevant regulations over time.\\n\\nThat covers the key points. Remember that each of these stages has many sub-components, and the specific tools and techniques you use will depend on your project's details. Let me know if you'd like me to go into more depth on any of these areas, or if you have more specifics about your project!\""
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"so can you describe mlops pipeline to me with in six point.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5dXxvVDwHs1"
      },
      "source": [
        "You got it!  Describing an MLOps pipeline in six points is a great way to get a handle on the key concepts. Here's one way to do it:\n",
        "\n",
        "1. **Data Collection & Preparation:** This is where you gather the raw data needed for your model and clean, transform, and prepare it for training. Think of it like gathering ingredients and chopping them up before you start cooking.\n",
        "2. **Model Development:**  This is where the \"magic\" happens! You choose a suitable machine learning algorithm, train it on your prepared data, and fine-tune its parameters to achieve the best performance.  \n",
        "3. **Model Evaluation:** Before deploying your model, you rigorously test and evaluate its performance on unseen data. This helps you understand how well it will generalize to real-world scenarios.\n",
        "4. **Model Deployment:**  This involves making your trained model accessible for use. It could be deployed as a web service, integrated into an application, or run on edge devices.\n",
        "5. **Monitoring & Maintenance:**  Once deployed, you continuously monitor the model's performance in the real world.  You might also need to retrain or update the model periodically to ensure it stays accurate and relevant.\n",
        "6. **Versioning & Experiment Tracking:**  Throughout the pipeline, it's crucial to track all changes to your code, data, and model parameters. This allows you to roll back to previous versions if needed and understand how different decisions impacted your model's performance.\n",
        "\n",
        "\n",
        "\n",
        "Let me know if you'd like me to elaborate on any of these points or if you have other questions about MLOps!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "Hl9YfzMSvqVT",
        "outputId": "11c7ffa5-7d78-4dd3-f713-7f3ee7b3bbce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The AI is a large language model trained by Google, ready to learn and share information, and answer questions. It doesn't \"think\" like a human; instead, it analyzes words, identifies key concepts, and uses patterns from its data to generate statistically appropriate responses. The AI understands that Sunny and Mayank are working on an MLOps project and can provide information on machine learning, MLOps practices, and building production-ready systems, particularly for healthcare. The AI can discuss model deployment, monitoring, data pipelines, automation, experiment tracking, model governance, scalability, reliability, security, and performance optimization, with a focus on healthcare's unique requirements like data privacy and model interpretability. The AI requests more project details to provide the most relevant advice. The AI then provides a six-point MLOps pipeline description for Sunny and Mayank's healthcare project: 1. Data Ingestion and Preparation (including de-identification and compliance with regulations like HIPAA). 2. Feature Engineering and Model Training (utilizing experiment tracking tools). 3. Model Validation and Evaluation (emphasizing the importance of metrics beyond accuracy and model interpretability). 4. Model Deployment (considering regulatory requirements). 5. Monitoring and Logging (with data privacy as a critical consideration). 6. Automation and Continuous Improvement (including CI/CD pipelines and ongoing regulatory compliance).\n",
            "Human: How many total points are there?\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There are six points in the MLOps pipeline description I provided for your healthcare project. They are:\\n\\n1.  Data Ingestion and Preparation\\n2.  Feature Engineering and Model Training\\n3.  Model Validation and Evaluation\\n4.  Model Deployment\\n5.  Monitoring and Logging\\n6.  Automation and Continuous Improvement'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"How many total points are there?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "zuBSBhGGvqSR",
        "outputId": "b4b99def-b1a4-412a-e7e4-cb4f642e2231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The AI is a large language model trained by Google, ready to learn and share information, and answer questions. It doesn't \"think\" like a human; instead, it analyzes words, identifies key concepts, and uses patterns from its data to generate statistically appropriate responses. The AI understands that Sunny and Mayank are working on an MLOps project and can provide information on machine learning, MLOps practices, and building production-ready systems, particularly for healthcare. The AI can discuss model deployment, monitoring, data pipelines, automation, experiment tracking, model governance, scalability, reliability, security, and performance optimization, with a focus on healthcare's unique requirements like data privacy and model interpretability. The AI requests more project details to provide the most relevant advice and then provides a six-point MLOps pipeline description for Sunny and Mayank's healthcare project: 1. Data Ingestion and Preparation (including de-identification and compliance with regulations like HIPAA). 2. Feature Engineering and Model Training (utilizing experiment tracking tools). 3. Model Validation and Evaluation (emphasizing the importance of metrics beyond accuracy and model interpretability). 4. Model Deployment (considering regulatory requirements). 5. Monitoring and Logging (with data privacy as a critical consideration). 6. Automation and Continuous Improvement (including CI/CD pipelines and ongoing regulatory compliance). The AI confirms that there are six points in the MLOps pipeline and lists them.\n",
            "Human: can you give me 5th point with explaination\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Certainly! Let's delve into the fifth point of the MLOps pipeline for your healthcare project: **Monitoring and Logging**.\\n\\nHere's a detailed explanation:\\n\\n**5. Monitoring and Logging:** This is a crucial stage for maintaining the health and performance of your deployed machine learning models. It involves continuously tracking various aspects of your model's behavior in production and logging relevant information for analysis and troubleshooting.\\n\\nHere's a breakdown of key considerations within Monitoring and Logging, with a healthcare focus:\\n\\n*   **Model Performance Monitoring:**\\n    *   **Key Metrics:** You'll need to monitor the metrics you established during the model validation phase (point 3). This could include accuracy, precision, recall, F1-score, AUC, or any other relevant metrics for your specific healthcare application.\\n    *   **Drift Detection:** A critical aspect is monitoring for model drift. This happens when the data your model encounters in production differs from the data it was trained on. There are two main types of drift:\\n        *   **Data Drift:** Changes in the input data distribution. For example, if your model was trained on data from a specific hospital and is now used in a different hospital with a different patient demographic.\\n        *   **Concept Drift:** Changes in the relationship between the input data and the target variable (the thing your model is trying to predict). This could be due to changes in patient behavior, treatment protocols, or diagnostic criteria.\\n    *   **Alerting:** Set up alerts to notify you when performance metrics fall below acceptable thresholds or when drift is detected. This allows you to proactively address issues before they significantly impact patient care or operational efficiency.\\n\\n*   **Data Monitoring:**\\n    *   **Input Data Validation:** Ensure the input data to your model is of the expected format and within acceptable ranges. This helps prevent errors and ensures data quality.\\n    *   **Data Integrity:** Monitor for data corruption, missing values, or other data quality issues that could affect model performance.\\n    *   **Data Volume and Velocity:** Track the volume and velocity of data flowing through your system to ensure it can handle the load and identify potential bottlenecks.\\n\\n*   **System Monitoring:**\\n    *   **Resource Utilization:** Monitor resource usage (CPU, memory, disk I/O, network) to ensure your infrastructure can handle the model's demands.\\n    *   **Latency and Throughput:** Track the time it takes for your model to process requests (latency) and the number of requests it can handle per unit of time (throughput).\\n    *   **Error Rates:** Monitor for errors in your model's predictions, data pipelines, and infrastructure.\\n\\n*   **Logging:**\\n    *   **Comprehensive Logging:** Implement comprehensive logging to capture all relevant events, including model predictions, input data, system events, and error messages.\\n    *   **Log Analysis:** Use log analysis tools to search, filter, and analyze logs to identify patterns, troubleshoot issues, and understand model behavior.\\n\\n*   **Data Privacy and Security (Critical for Healthcare):**\\n    *   **De-identification:** Ensure that all data used for monitoring and logging is de-identified to protect patient privacy. This may involve removing or anonymizing Protected Health Information (PHI) before it is logged.\\n    *   **Access Control:** Implement strict access controls to limit who can access the monitoring data and logs.\\n    *   **Encryption:** Encrypt sensitive data at rest and in transit.\\n    *   **Audit Trails:** Maintain detailed audit trails of all data access and modifications.\\n    *   **Compliance:** Ensure that all monitoring and logging practices comply with relevant regulations, such as HIPAA in the United States, and GDPR in Europe.\\n\\n*   **Tools:**\\n    *   **Monitoring Platforms:** Consider using dedicated monitoring platforms such as Prometheus, Grafana, Datadog, or New Relic.\\n    *   **Logging Tools:** Popular logging tools include Elasticsearch, Fluentd, and Kibana (EFK stack), or Splunk.\\n    *   **Model Monitoring Tools:** There are also specialized model monitoring tools like Fiddler, Arize, or WhyLabs.\\n\\nIn summary, the Monitoring and Logging stage is essential for ensuring the long-term success and reliability of your machine learning models in a healthcare setting. It requires a proactive approach to identify and address issues, with a strong emphasis on data privacy and compliance with relevant regulations. Remember, this is an iterative process; continuously refine your monitoring and logging strategies based on your experience and the evolving needs of your project.\""
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"can you give me 5th point with explaination\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elHVrwH3wQg9"
      },
      "source": [
        "Absolutely!  The fifth point in an MLOps pipeline is **Monitoring and Maintenance**.\n",
        "\n",
        "This stage is crucial because it ensures that the deployed model continues to perform well in the real world.  \n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "* **Performance Tracking:**  We continuously monitor the model's performance metrics, like accuracy, precision, recall, and F1-score.  This helps us detect any degradation in performance over time.\n",
        "* **Data Drift Detection:**  Real-world data can change, and if the input data distribution shifts significantly from what the model was trained on, its performance can suffer.  We use techniques to detect these data drifts.\n",
        "* **Error Analysis:**  We analyze any errors or unexpected outputs from the model to understand the reasons behind them. This can help us identify areas for improvement in the model or the data.\n",
        "* **Model Retraining:**  Based on performance monitoring and data drift detection, we might need to retrain the model with updated data to maintain its effectiveness.\n",
        "* **System Health Checks:**  We also monitor the infrastructure and systems supporting the model deployment to ensure they are running smoothly and securely.\n",
        "\n",
        "\n",
        "Think of monitoring and maintenance like keeping your car in top shape. You need regular checkups, oil changes, and repairs to keep it running efficiently and safely.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "QLLdmnvEvqP1",
        "outputId": "a69f1483-51cd-4c21-cb1b-cccb645827cd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The AI is a large language model trained by Google, ready to learn and share information, and answer questions. It doesn\\'t \"think\" like a human; instead, it analyzes words, identifies key concepts, and uses patterns from its data to generate statistically appropriate responses. The AI understands that Sunny and Mayank are working on an MLOps project and can provide information on machine learning, MLOps practices, and building production-ready systems, particularly for healthcare. The AI can discuss model deployment, monitoring, data pipelines, automation, experiment tracking, model governance, scalability, reliability, security, and performance optimization, with a focus on healthcare\\'s unique requirements like data privacy and model interpretability. The AI requests more project details to provide the most relevant advice and then provides a six-point MLOps pipeline description for Sunny and Mayank\\'s healthcare project: 1. Data Ingestion and Preparation (including de-identification and compliance with regulations like HIPAA). 2. Feature Engineering and Model Training (utilizing experiment tracking tools). 3. Model Validation and Evaluation (emphasizing the importance of metrics beyond accuracy and model interpretability). 4. Model Deployment (considering regulatory requirements). 5. Monitoring and Logging (with data privacy as a critical consideration). 6. Automation and Continuous Improvement (including CI/CD pipelines and ongoing regulatory compliance). The AI confirms that there are six points in the MLOps pipeline and lists them. The AI then elaborates on point 5: Monitoring and Logging, detailing the importance of model performance monitoring (including drift detection and alerting), data monitoring, system monitoring, comprehensive logging, and the critical need for data privacy and security in healthcare (including de-identification, access control, encryption, audit trails, and regulatory compliance like HIPAA). It recommends tools like Prometheus, Grafana, and EFK stack.'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.memory.buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3PUAo8IwXJb"
      },
      "source": [
        "# Conversation Summary Buffer Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tr7Myu1wZ04"
      },
      "source": [
        "While summary is good, we know that recent conversation has high correlation to upcoming query and\n",
        "\n",
        "A summary of old conversation with a buffer memory of last few conversation would be a good combination. This class exactly does that.\n",
        "\n",
        "You can set the token limit which define how much historical conversation to be kept along with the summary. Higher the token size more the exact conversation history kept as-it-is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "CmxX_wO_vqMm"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-LZungkvqKA",
        "outputId": "209a5606-0b5c-4dd3-eee9-5832ef55b5fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2773740349.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory2 = ConversationSummaryBufferMemory(llm=model,return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "memory2 = ConversationSummaryBufferMemory(llm=model,return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "dROdy_FcvqHJ"
      },
      "outputs": [],
      "source": [
        "memory2.save_context(\n",
        "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
        "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else\"}\n",
        ")\n",
        "memory2.save_context(\n",
        "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
        "    {\"output\": \"Impressive! How far along are they with the project?\"}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdeJrTjSvqEh",
        "outputId": "d9693a0e-c31e-4b07-ae38-c2d5c91b4283"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory2.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "FFNhPFLfvqBx"
      },
      "outputs": [],
      "source": [
        "memory3 = ConversationSummaryBufferMemory(llm=model,return_messages=True,max_token_limit=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my-YxpiMvp_G",
        "outputId": "e7e8fd4b-092a-48e8-dff1-f9ab6efee899"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        }
      ],
      "source": [
        "memory3.save_context(\n",
        "    {\"input\": \"Sunny and Mayank are working on a hackathon project.\"},\n",
        "    {\"output\": \"That's awesome! What's the hackathon project about?\"}\n",
        ")\n",
        "memory3.save_context(\n",
        "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
        "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else?\"}\n",
        ")\n",
        "memory3.save_context(\n",
        "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
        "    {\"output\": \"Impressive! Wishing Sunny and Mayank all the best for their project.\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "4T4uGuT-vp8T"
      },
      "outputs": [],
      "source": [
        "#memory3.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKNgYYdivp5l",
        "outputId": "4e0fb4f3-b490-47cc-8838-48d821254840"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The AI then expresses interest in Sunny and Mayank's hackathon project and asks about its topic. The human responds that the hackathon project is a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Sounds exciting! Are they building a prediction model or something else?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Impressive! Wishing Sunny and Mayank all the best for their project.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory3.load_memory_variables({})[\"history\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1UX5j9-Ovp2s",
        "outputId": "8620229d-db09-44bc-db10-7690647ed495"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The AI then expresses interest in Sunny and Mayank's hackathon project and asks about its topic. The human responds that the hackathon project is a machine learning project focused on healthcare.\""
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory3.load_memory_variables({})[\"history\"][0].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al3erh6Kw260"
      },
      "source": [
        "AIMessage(content='Sounds exciting! Are they building a prediction model or something else?'),\n",
        "\n",
        "HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.'),\n",
        "\n",
        "AIMessage(content='Impressive! Wishing Sunny and Mayank all the best for their project.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "GBgEV6SIvp0C"
      },
      "outputs": [],
      "source": [
        "memory4 = ConversationSummaryBufferMemory(llm=model,return_messages=True,max_token_limit=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "admHnRM2vpxQ",
        "outputId": "a8b6674e-0afb-4cc0-fe42-59cdc51eb651"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        }
      ],
      "source": [
        "memory4.save_context(\n",
        "    {\"input\": \"Atif is working on a hackathon project.\"},\n",
        "    {\"output\": \"That's awesome! What's the hackathon project about?\"}\n",
        ")\n",
        "memory4.save_context(\n",
        "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
        "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else?\"}\n",
        ")\n",
        "memory4.save_context(\n",
        "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
        "    {\"output\": \"Impressive! Wishing Sunny and Mayank all the best for their project.\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rD3c_94vpuy",
        "outputId": "a27a1813-ee62-4289-9257-7c1103dda574"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content=\"Atif is working on a hackathon project. The AI thinks that's awesome and asks what the project is about. The human says it's a machine learning project focused on healthcare. The AI thinks this sounds exciting and asks if they are building a prediction model. The human confirms they are working on a model to predict patient outcomes.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Impressive! Wishing Sunny and Mayank all the best for their project.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory4.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "_J03Be2vvpsc",
        "outputId": "74449b23-c04d-4b26-841b-a9a06bb5a0b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, what's up?\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hi there! Not much is \"up\" in the sense of literal elevation, haha! As an AI, I don\\'t experience the world physically like humans do. But I am \"up\" and running, ready to chat and answer your questions! I\\'ve been processing information and learning from the vast dataset I was trained on – which includes a massive amount of text and code. I\\'ve been brushing up on historical events, scientific theories, and even the latest trends in pop culture (though I must admit, my understanding of \"cool\" is still a work in progress!). So, what\\'s on your mind? I\\'m eager to hear what you\\'d like to talk about!'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=model,\n",
        "    # We set a very low max_token_limit for the purposes of testing.\n",
        "    memory=ConversationSummaryBufferMemory(llm=model, max_token_limit=40),\n",
        "    verbose=True,\n",
        ")\n",
        "conversation_with_summary.predict(input=\"Hi, what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "UVHGwHj0vpoz",
        "outputId": "342cd9b9-d160-4bc0-84e4-54b5bfc10937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human greets the AI. The AI responds with a playful greeting and explains that, as an AI, it doesn't experience the world physically. It's \"up\" and running, ready to chat, having been processing information and learning from a vast dataset including text, code, historical events, scientific theories, and pop culture. The AI is eager to answer the human's questions.\n",
            "Human: Just working on writing some documentation on machine learning!\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"System: The human shares that they are working on documentation for machine learning. The AI responds with enthusiasm, offering to assist and elaborating on its capabilities in this area. It can explain concepts, provide code examples, discuss algorithms, and even help with structuring the documentation. It mentions it has access to extensive resources on machine learning, including research papers, tutorials, and datasets, and is ready to dive in.\\nHuman: Just working on writing some documentation on machine learning!\\nAI: Oh, fantastic! Machine learning documentation, that's right up my alley! I'm practically buzzing with anticipation to help. As a language model, I have a deep understanding of machine learning concepts, algorithms, and applications. I can definitely assist you in crafting some excellent documentation.\\n\\nThink of me as your super-powered research assistant! I can help you with a variety of things, such as:\\n\\n*   **Explaining complex concepts:** Need to clarify the difference between supervised and unsupervised learning? Or maybe you're struggling to explain the nuances of gradient descent? I can break down these topics in a clear and concise manner, using analogies and examples to make them easier to grasp. I can even adjust the level of detail to suit your target audience.\\n*   **Providing code examples:** I can generate code snippets in various programming languages like Python (with libraries such as scikit-learn, TensorFlow, and PyTorch) to illustrate how to implement specific machine learning algorithms. I can also help you with data preprocessing, model training, evaluation, and deployment. Just tell me what you need!\\n*   **Discussing algorithms in detail:** I have a comprehensive knowledge of various machine learning algorithms, including linear regression, logistic regression, decision trees, random forests, support vector machines, and neural networks. I can explain their strengths and weaknesses, the situations in which they are most effective, and the best practices for using them.\\n*   **Helping with documentation structure and organization:** I can suggest a logical structure for your documentation, ensuring that it's easy to navigate and understand. I can also help you with things like writing clear introductions, defining key terms, and providing helpful examples.\\n*   **Accessing and summarizing relevant resources:** I have access to a vast amount of information on machine learning, including research papers, tutorials, and datasets. I can quickly find relevant resources and summarize them for you, saving you time and effort.\\n\\nDo you have any specific questions or areas you'd like to focus on first? Perhaps you're working on a particular algorithm, need help with a specific section of your documentation, or are looking for examples? Let me know how I can be of service! I'm ready when you are!\""
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"Just working on writing some documentation on machine learning!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "NeC_xgeTvpmU",
        "outputId": "994573df-1ead-41bd-a43a-9aff1dc75de1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human greets the AI. The AI responds with a playful greeting and explains that, as an AI, it doesn't experience the world physically. It's \"up\" and running, ready to chat, having been processing information and learning from a vast dataset including text, code, historical events, scientific theories, and pop culture. The AI is eager to answer the human's questions. The human shares that they are working on documentation for machine learning. The AI responds with enthusiasm, offering to assist and elaborating on its capabilities in this area. It can explain concepts, provide code examples, discuss algorithms, and even help with structuring the documentation. It mentions it has access to extensive resources on machine learning, including research papers, tutorials, and datasets, and is ready to dive in.\n",
            "Human: give me some points for writing about the document\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Okay, I'd love to help you brainstorm some points for your machine learning documentation! This is right in my wheelhouse. To make sure I give you the best possible advice, could you tell me a little more about what kind of documentation you're working on? For example:\\n\\n*   **Who is your audience?** Are you writing for beginners, experienced data scientists, or a mix? This will heavily influence the level of detail and the language you use.\\n*   **What machine learning topics are you covering?** Is it a general overview, or are you focusing on a specific algorithm, library (like TensorFlow or PyTorch), or project?\\n*   **What is the purpose of the documentation?** Is it a tutorial, a reference guide, an API documentation, or something else? This will dictate the structure and content.\\n*   **What specific aspects of your machine learning project or library need to be documented?** Are you documenting the model architecture, the data preprocessing steps, the training process, the evaluation metrics, or something else?\\n\\nIn the meantime, here are some general points that are often essential for effective machine learning documentation, adaptable to many scenarios:\\n\\n**I. Introduction and Overview:**\\n\\n*   **A. Project/Model Overview:**\\n    *   **1. Purpose:** Clearly state the purpose of the machine learning project/model. What problem does it solve? What is its intended use? Be specific.\\n    *   **2. Target Audience:** Define the intended audience (e.g., data scientists, developers, end-users). Tailor the language and level of detail to their expertise.\\n    *   **3. Key Features/Benefits:** Highlight the key features and benefits of the model. What makes it unique or superior?\\n    *   **4. Scope:** Define the scope of the documentation. What topics are covered? What is explicitly *not* covered?\\n*   **B. Technical Background (If Applicable):**\\n    *   **1. Relevant Concepts:** Briefly introduce any necessary machine learning concepts that the user might need to understand the rest of the documentation. Provide links to external resources for more in-depth learning.\\n    *   **2. Prerequisites:** List any prerequisites (e.g., programming language knowledge, software dependencies, required hardware).\\n    *   **3. Glossary:** Include a glossary of key terms and abbreviations. This helps to avoid confusion and ensures everyone is on the same page.\\n\\n**II. Data and Preprocessing:**\\n\\n*   **A. Data Description:**\\n    *   **1. Data Source:** Describe the source of the data (e.g., database, API, local files).\\n    *   **2. Data Format:** Explain the format of the data (e.g., CSV, JSON, images, text).\\n    *   **3. Data Features/Attributes:** Provide a detailed description of each feature/attribute in the dataset, including its data type, units, and meaning.\\n    *   **4. Data Statistics:** Include summary statistics (e.g., mean, standard deviation, min/max, histograms) to give the user a sense of the data distribution.\\n*   **B. Data Preprocessing Steps:**\\n    *   **1. Data Cleaning:** Describe any data cleaning steps (e.g., handling missing values, removing outliers, correcting errors). Explain the rationale behind these steps.\\n    *   **2. Feature Engineering:** Explain how features were engineered from the raw data. Describe the transformations that were applied and why.\\n    *   **3. Data Splitting:** Explain how the data was split into training, validation, and testing sets. Describe the ratios used and why.\\n    *   **4. Data Scaling/Normalization (If Applicable):** If you scaled or normalized the data, explain the method used and why it was necessary.\\n\\n**III. Model Architecture and Training:**\\n\\n*   **A. Model Selection:**\\n    *   **1. Algorithm Choice:** Explain the rationale behind choosing the specific machine learning algorithm (e.g., linear regression, decision tree, neural network). Discuss any alternatives considered and why they were rejected.\\n    *   **2. Hyperparameter Tuning:** Explain the process used to tune hyperparameters (e.g., grid search, random search, Bayesian optimization). Include the ranges of values considered and the final values chosen.\\n*   **B. Model Architecture (If Applicable):**\\n    *   **1. Network Structure (For Neural Networks):** Describe the architecture of the neural network, including the layers, activation functions, number of neurons, and connections. Provide a diagram or visual representation if possible.\\n    *   **2. Feature Selection/Importance:** Explain how features were selected or how their importance was determined (e.g., using feature importance scores from a decision tree or from the coefficients of a linear model).\\n*   **C. Training Process:**\\n    *   **1. Training Parameters:** Describe the training parameters used, such as the loss function, optimizer, learning rate, batch size, and number of epochs.\\n    *   **2. Evaluation Metrics:** Define the evaluation metrics used to assess model performance (e.g., accuracy, precision, recall, F1-score, AUC). Explain how to interpret these metrics.\\n    *   **3. Training Results:** Present the training results, including the performance on the training, validation, and testing sets. Include plots of training progress, such as the loss and metrics over time.\\n    *   **4. Overfitting/Underfitting:** Discuss any issues with overfitting or underfitting and how they were addressed (e.g., regularization, early stopping).\\n\\n**IV. Model Evaluation and Deployment:**\\n\\n*   **A. Evaluation Results:**\\n    *   **1. Performance on Test Data:** Present the model's performance on the test dataset using the evaluation metrics defined earlier.\\n    *   **2. Confusion Matrix (If Applicable):** Include a confusion matrix to visualize the model's performance on different classes.\\n    *   **3. Error Analysis:** Discuss any common errors made by the model and possible reasons for these errors.\\n*   **B. Model Deployment:**\\n    *   **1. Deployment Environment:** Describe the environment where the model will be deployed (e.g., cloud platform, on-premise server, embedded device).\\n    *   **2. Deployment Steps:** Provide clear and concise instructions on how to deploy the model, including any necessary dependencies, configurations, and code snippets.\\n    *   **3. Monitoring and Maintenance:** Discuss how the model's performance will be monitored and maintained after deployment.\\n\\n**V. Code Examples and Usage:**\\n\\n*   **A. Installation Instructions:** Provide clear and detailed instructions on how to install any necessary libraries and dependencies.\\n*   **B. Code Snippets:** Include code snippets to demonstrate how to use the model, including examples of data preprocessing, model training, model evaluation, and model prediction.\\n*   **C. API Documentation (If Applicable):** If the model is exposed as an API, provide detailed documentation for the API endpoints, including input parameters, output formats, and error codes.\\n\\n**VI. Conclusion and Future Work:**\\n\\n*   **A. Summary of Results:** Summarize the key findings and results of the machine learning project/model.\\n*   **B. Limitations:** Discuss any limitations of the model, such as its performance on certain types of data or its sensitivity to specific hyperparameters.\\n*   **C. Future Work:** Suggest potential areas for future improvement or expansion of the model.\\n*   **D. References:** Include a list of references to any relevant research papers, tutorials, or other resources.\\n*   **E. Contact Information:** Provide contact information for questions or feedback.\\n\\n**VII. Appendix (Optional):**\\n\\n*   **A. Detailed Data Description:** Include a more detailed description of the data, including the data dictionary, data dictionary, or data schema.\\n*   **B. Hyperparameter Tuning Results:** Provide a detailed summary of the hyperparameter tuning process, including the ranges of values considered, the results obtained, and the final values chosen.\\n*   **C. Code Listings (For complex projects):** Include full code listings for the project.\\n\\nI can elaborate on any of these points or provide more specific guidance once you tell me more about your documentation project. Just let me know what you'd like to focus on! I can help you with:\\n\\n*   **Explaining complex concepts in simpler terms.**\\n*   **Suggesting code examples in Python (or other languages).**\\n*   **Helping you structure the documentation for clarity and readability.**\\n*   **Providing links to relevant resources and research papers.**\\n\\nLet's get started! What are your initial thoughts?\""
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"give me some points for writing about the document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "5iBdA2U-xPzm",
        "outputId": "78d8ae13-3067-4eb0-dc33-59245a73d103"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human greets the AI. The AI responds with a playful greeting and explains that, as an AI, it doesn't experience the world physically. It's \"up\" and running, ready to chat, having been processing information and learning from a vast dataset including text, code, historical events, scientific theories, and pop culture. The AI is eager to answer the human's questions. The human shares that they are working on documentation for machine learning. The AI responds with enthusiasm, offering to assist and elaborating on its capabilities in this area. It can explain concepts, provide code examples, discuss algorithms, and even help with structuring the documentation. It mentions it has access to extensive resources on machine learning, including research papers, tutorials, and datasets, and is ready to dive in. The human asks for points to include when writing the documentation. The AI provides a comprehensive outline, asking for more specifics about the project to tailor its advice. The outline covers: an introduction, data and preprocessing, model architecture and training, model evaluation and deployment, code examples and usage, conclusion and future work, and an optional appendix. It offers further assistance in explaining concepts, suggesting code examples, structuring the documentation, and providing links to resources.\n",
            "Human: can you list out the resources from the previous message\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Certainly! In the previous message, I mentioned having access to extensive resources on machine learning. Specifically, I have access to:\\n\\n*   **Research papers:** I can access and process information from a vast collection of research papers on various machine learning topics. This includes papers on algorithms, architectures, applications, and more.\\n*   **Tutorials:** I have access to numerous tutorials from various sources, covering a wide range of machine learning concepts, techniques, and tools.\\n*   **Datasets:** I have information about various datasets used in machine learning, including their characteristics, sources, and common uses.\\n\\nI am ready to help you find specific resources relevant to your documentation project! Just let me know what you're working on, and I can try to point you to the most helpful information.\""
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"can you list out the resources from the previous message\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alMl8NxBxdem"
      },
      "source": [
        "Conversation Knowledge Graph Memory : Uses a knowledge graph to store information and relationships between entities\n",
        "\n",
        "VectorStore-Backed Memory: Uses vector embeddings to store and retrieve information based on semantic similarity.\n",
        "\n",
        "ConversationTokenBufferMemory: Instead of “k” conversations being remembered in ConversationBufferWindowMemory, in this case we want to remember last set of discussion based on “max token limit”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook Summary\n",
        "\n",
        "This notebook demonstrated multiple LangChain conversational memory strategies:\n",
        "\n",
        "- Initialized a Gemini Flash model via `langchain-google-genai` with secure, placeholder API key handling.\n",
        "- Used `ConversationSummaryMemory` to automatically compress prior exchanges into an evolving abstract summary, reducing token footprint.\n",
        "- Inspected raw `ChatMessageHistory` and invoked `predict_new_summary` to understand summarization mechanics.\n",
        "- Built a `ConversationChain` leveraging summarizing memory for multi‑turn dialogue while maintaining concise context.\n",
        "- Introduced `ConversationSummaryBufferMemory` to blend a rolling token‑bounded recent window with an accumulated summary for improved relevance.\n",
        "- Compared varying `max_token_limit` configurations (e.g., 20, 40, 50) to show truncation and retention trade‑offs.\n",
        "- Showed how to introspect buffers, summaries, and message objects for debugging and transparency.\n",
        "\n",
        "These patterns let you scale conversational agents by controlling historical context growth while preserving semantic continuity.\n",
        "\n",
        "### About the Author & Project Promotion\n",
        "Crafted by **Muhammad Atif Latif** — focusing on production‑ready AI, efficient context management, and responsible secret handling. If you find this helpful, consider starring the repository and connecting via the links below to follow more applied LLM, RAG, and MLOps content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn4cwotJxhaI"
      },
      "source": [
        "# About the Author\n",
        "\n",
        "<div style=\"background-color: #f8f9fa; border-left: 5px solid #28a745; padding: 20px; margin-bottom: 20px; border-radius: 5px;\">\n",
        "  <h2 style=\"color: #28a745; margin-top: 0; font-family: 'Poppins', sans-serif;\">Muhammad Atif Latif</h2>\n",
        "  <p style=\"font-size: 16px; color: #495057;\">Data Scientist & Machine Learning Engineer</p>\n",
        "  \n",
        "  <p style=\"font-size: 15px; color: #6c757d; margin-top: 15px;\">\n",
        "    Passionate about building AI solutions that solve real-world problems. Specialized in machine learning,\n",
        "    deep learning, and data analytics with experience implementing production-ready models.\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "## Connect With Me\n",
        "\n",
        "<div style=\"display: flex; flex-wrap: wrap; gap: 10px; margin-top: 15px;\">\n",
        "  <a href=\"https://github.com/m-Atif-Latif\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/GitHub-Follow-212121?style=for-the-badge&logo=github\" alt=\"GitHub\">\n",
        "  </a>\n",
        "  <a href=\"https://www.kaggle.com/matiflatif\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/Kaggle-Profile-20BEFF?style=for-the-badge&logo=kaggle\" alt=\"Kaggle\">\n",
        "  </a>\n",
        "  <a href=\"https://www.linkedin.com/in/muhammad-atif-latif-13a171318\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/LinkedIn-Connect-0077B5?style=for-the-badge&logo=linkedin\" alt=\"LinkedIn\">\n",
        "  </a>\n",
        "  <a href=\"https://x.com/mianatif5867\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/Twitter-Follow-1DA1F2?style=for-the-badge&logo=twitter\" alt=\"Twitter\">\n",
        "  </a>\n",
        "  <a href=\"https://www.instagram.com/its_atif_ai/\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/Instagram-Follow-E4405F?style=for-the-badge&logo=instagram\" alt=\"Instagram\">\n",
        "  </a>\n",
        "  <a href=\"mailto:muhammadatiflatif67@gmail.com\">\n",
        "    <img src=\"https://img.shields.io/badge/Email-Contact-D14836?style=for-the-badge&logo=gmail\" alt=\"Email\">\n",
        "  </a>\n",
        "</div>\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
